{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 추가\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), '..')))\n",
    "\n",
    "from app.utils import get_embedding_model\n",
    "\n",
    "\n",
    "# 변수 선언\n",
    "ROOT_PATH = '/Users/passion1014/project/langchain/rag_server'\n",
    "INDEX_ID = \"SAMPLE_simple\" # VectorDB INDEX 명\n",
    "FILE_NAME = f'{INDEX_ID}.simple' # 파일이름\n",
    "\n",
    "# 임베딩 선언\n",
    "embeddings = get_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m chunks \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(table_pattern, text, re\u001b[38;5;241m.\u001b[39mDOTALL)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# 각 chunk를 문서 객체로 변환\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43mtext_splitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 각 chunk를 문서 객체로 변환 이 과정에서 각 청크를 개별 문서로 다룬다\u001b[39;00m\n\u001b[1;32m     25\u001b[0m docs \u001b[38;5;241m=\u001b[39m [Document(page_content\u001b[38;5;241m=\u001b[39mchunk) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks]\n",
      "File \u001b[0;32m~/anaconda3/envs/ragserver/lib/python3.12/site-packages/langchain_text_splitters/base.py:79\u001b[0m, in \u001b[0;36mTextSplitter.create_documents\u001b[0;34m(self, texts, metadatas)\u001b[0m\n\u001b[1;32m     77\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     78\u001b[0m previous_chunk_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     80\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(_metadatas[i])\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_start_index:\n",
      "File \u001b[0;32m~/anaconda3/envs/ragserver/lib/python3.12/site-packages/langchain_text_splitters/character.py:26\u001b[0m, in \u001b[0;36mCharacterTextSplitter.split_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# First we naively split the large input into a bunch of smaller ones.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m separator \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_separator \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_separator_regex \u001b[38;5;28;01melse\u001b[39;00m re\u001b[38;5;241m.\u001b[39mescape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_separator)\n\u001b[1;32m     25\u001b[0m )\n\u001b[0;32m---> 26\u001b[0m splits \u001b[38;5;241m=\u001b[39m \u001b[43m_split_text_with_regex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseparator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_keep_separator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m _separator \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keep_separator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_separator\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_splits(splits, _separator)\n",
      "File \u001b[0;32m~/anaconda3/envs/ragserver/lib/python3.12/site-packages/langchain_text_splitters/character.py:52\u001b[0m, in \u001b[0;36m_split_text_with_regex\u001b[0;34m(text, separator, keep_separator)\u001b[0m\n\u001b[1;32m     46\u001b[0m         splits \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     47\u001b[0m             (splits \u001b[38;5;241m+\u001b[39m [_splits[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m     48\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m keep_separator \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m ([_splits[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m+\u001b[39m splits)\n\u001b[1;32m     50\u001b[0m         )\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         splits \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseparator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     splits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(text)\n",
      "File \u001b[0;32m~/anaconda3/envs/ragserver/lib/python3.12/re/__init__.py:207\u001b[0m, in \u001b[0;36msplit\u001b[0;34m(pattern, string, maxsplit, flags)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit\u001b[39m(pattern, string, maxsplit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    200\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Split the source string by the occurrences of the pattern,\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    returning a list containing the resulting substrings.  If\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m    capturing parentheses are used in pattern, then the text of all\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    and the remainder of the string is returned as the final element\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m    of the list.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxsplit\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object, got 'tuple'"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from dotenv import load_dotenv\n",
    "from app.vectordb import upload_vectordb\n",
    "from langchain.docstore.document import Document\n",
    "import re\n",
    "\n",
    "\n",
    "# 파일 내용 로드 및 처리\n",
    "text = upload_vectordb.load_and_detect_encoding(f'{ROOT_PATH}/data/{FILE_NAME}')\n",
    "\n",
    "\n",
    "# 2. 의미별로 chunk로 나누기\n",
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
    "# chunks = text_splitter.split_text(text)\n",
    "\n",
    "table_pattern = r'### (.*?)\\((.*?)\\)\\s*\\{(.*?)\\}'\n",
    "chunks = re.findall(table_pattern, text, re.DOTALL)\n",
    "\n",
    "\n",
    "# 각 chunk를 문서 객체로 변환\n",
    "docs = text_splitter.create_documents(chunks)\n",
    "\n",
    "# 각 chunk를 문서 객체로 변환 이 과정에서 각 청크를 개별 문서로 다룬다\n",
    "docs = [Document(page_content=chunk) for chunk in chunks]\n",
    "\n",
    "# FAISS 인덱스 생성 및 문서 추가\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# FAISS 인덱스 저장\n",
    "db.save_local(f'{ROOT_PATH}/data/vector/{INDEX_ID}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragserver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
