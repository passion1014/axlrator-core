from datetime import datetime
import json
import re
import time
import uuid

from typing import Annotated, List, Optional, Sequence, TypedDict

from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage, ToolMessage
from langchain_core.runnables import RunnableConfig
from langchain_core.tools import tool

from langfuse import Langfuse
from langfuse.callback import CallbackHandler

from langgraph.graph import END, StateGraph
from langgraph.graph.message import add_messages
from langgraph.types import StreamWriter

from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm.attributes import flag_modified

from axlrator_core.db_model.axlrui_database import get_axlrui_session
from axlrator_core.db_model.axlrui_database_models import Chat
from axlrator_core.utils import get_llm_model
from axlrator_core.vectordb.bm25_search import ElasticsearchBM25
from axlrator_core.vectordb.vector_store import get_vector_store
from axlrator_core.logger import logger



class CodeChatState(TypedDict):
    """The state of the agent."""
    thread_id: str
    chat_type:str # 01: ê²€ìƒ‰ì–´ ì°¾ê¸°, 02: ì§ˆë¬¸í•˜ê¸°, 03: í›„ì†ì§ˆë¬¸í•˜ê¸°, 04: ì œëª©ì§“ê¸°, 05: íƒœê·¸ ì¶”ì¶œ
    messages: Annotated[Sequence[BaseMessage], add_messages] # add_messages is a reducer
    context_datas: Optional[List[dict]] = None # file_contexts, files, vectordatasë¥¼ ëª¨ì€ context ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    context:str
    metadata:Optional[dict] = None
    is_vector_search:str
    response:str
    

class CodeChatAgent:
    def __init__(self, index_name:str="cg_code_assist"):
        self.index_name = index_name
        self.langfuse = Langfuse()
        
    @classmethod
    async def create(cls, index_name: str, session: AsyncSession, config) -> 'CodeChatAgent':
        # __init__ í˜¸ì¶œ
        instance = cls(index_name)
        
        instance.db_session = session
        # instance.faissVectorDB = await get_vector_db(collection_name=index_name, session=session)
        # instance.es_bm25 = ElasticsearchBM25(index_name=index_name)
        
        instance.callback_handlers = config.get("callbacks")
        instance.model = get_llm_model()
        # instance.model = get_llm_model().with_config(callbacks=[callback_handler])
        
        
        return instance


    @staticmethod
    def get_last_user_message(state: CodeChatState) -> Optional[str]:
        """
        Retrieves the last message from the user (HumanMessage) in the chat history.
        
        Returns:
            The content of the last HumanMessage, or None if not found.
        """
        for msg in reversed(state["messages"]):
            if isinstance(msg, HumanMessage):
                return msg.content.strip()
        return None

    @staticmethod
    def format_chat_history(state: CodeChatState) -> str:
        """
        Formats the chat history for display, ensuring proper indentation and separation.
        """
        chat_lines = []
        indent = "  "
        recent_messages = state["messages"][-3:]
        for msg in recent_messages:
            if isinstance(msg, HumanMessage):
                chat_lines.append(f"{indent}USER: {msg.content.strip()}")
            elif isinstance(msg, AIMessage):
                chat_lines.append(f"{indent}ASSISTANT: {msg.content.strip()}")
        # Join messages with a blank line for separation
        chat_history = "<chat_history>\n" + "\n\n".join(chat_lines) + "\n</chat_history>"
        return chat_history

    def pre_process_node(self, state: CodeChatState) -> CodeChatState:
        # context_datasì˜ í•­ëª©ì¤‘ì—ì„œ typeì´ filesì¸ í•­ëª©ë“¤ì€ contentë¥¼ ì¡°íšŒí•´ì„œ ì…‹íŒ…í•œë‹¤.
        context_datas = state.get("context_datas", [])
        for i, file in enumerate(context_datas):
            if file.get("type") == "file":
                file_data = self.get_file_context(file["id"])  # file dbì—ì„œ ì¡°íšŒ
                if file_data:
                    file["content"] = file_data.get("content", "").strip() or ""  # ì¡°íšŒëœ content ì…‹íŒ…
        
        state["context_datas"] = context_datas
        return state

    def check_added_file_context(self, context_datas: Optional[List[dict]]) -> bool:
        """
        context_datasì— 'type'ì´ 'selection' ë˜ëŠ” 'file'ì¸ í•­ëª©ì´ ì¡´ì¬í•˜ëŠ”ì§€ ì—¬ë¶€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        Args:
            context_datas (Optional[List[dict]]): ì»¨í…ìŠ¤íŠ¸ ë°ì´í„° ëª©ë¡

        Returns:
            bool: í•´ë‹¹ í•­ëª©ì´ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ True, ì—†ìœ¼ë©´ False
        """
        if not context_datas:
            return False

        for item in context_datas:
            if item.get("type") in ("selection", "file"):
                return True
        return False

    def merge_context_datas_node(self, state: CodeChatState) -> CodeChatState:
        parts = []
        context_datas = state.get("context_datas", [])
        
        if isinstance(context_datas, list):
            for i, context_data in enumerate(context_datas):
                if context_data:
                    _content = context_data.get("content", "")
                    if _content.strip():
                        parts.append(
                            f"<source id=\"{i+1}\" name=\"{context_data['name']}\">\n{_content}\n</source>"
                            # id= ì…‹íŒ…ë˜ëŠ” ê°’ì„ ë³„ë„ì˜ seqë¡œ ê´€ë¦¬í•˜ëŠ”ê²Œ ì¢‹ì„ë“¯.
                        )

        if parts:
            content = "\n\n".join(parts)
            state["context"] = content
        
        return state

    def get_file_context(self, file_id: str) -> Optional[dict]:
        from axlrator_core.db_model.axlrui_database import get_axlrui_session
        from axlrator_core.db_model.axlrui_database_models import File

        with get_axlrui_session() as session:
            file_row = session.query(File).filter(File.id == file_id).first()
            if file_row and file_row.data:
                try:
                    # contentê°€ ë¬¸ìì—´ì¸ì§€ í™•ì¸í•˜ê³ , ì¸ì½”ë”© ë¬¸ì œ ì˜ˆë°©ì„ ìœ„í•œ ë³´ì •
                    data = file_row.data
                    if isinstance(data, str):
                        data = json.loads(data)

                    if isinstance(data, bytes):
                        data = json.loads(data.decode("utf-8", errors="replace"))

                    elif not isinstance(data, dict):
                        data = {}

                    return data
                except Exception as e:
                    # ë¡œê¹…ì„ ì¶”ê°€í•˜ê³  None ë°˜í™˜
                    logger.info(f"Error parsing file data: {e}")
                    return None
        return None


    @tool
    def get_weather(self, location: str):
        """Call to get the weather from a specific location."""
        if any([city in location.lower() for city in ["sf", "san francisco"]]):
            return "It's sunny in San Francisco, but you better look out if you're a Gemini ğŸ˜ˆ."
        else:
            return f"I am not sure what the weather is in {location}"


    # Define our tool node
    def tool_node(self, state: CodeChatState):
        tools = [self.get_weather]
        tools_by_name = {tool.name: tool for tool in tools}

        outputs = []
        for tool_call in state["messages"][-1].tool_calls:
            tool_result = tools_by_name[tool_call["name"]].invoke(tool_call["args"])
            outputs.append(
                ToolMessage(
                    content=json.dumps(tool_result),
                    name=tool_call["name"],
                    tool_call_id=tool_call["id"],
                )
            )
        return {"messages": outputs}

    @staticmethod
    def convert_to_messages(chat_prompts_raw):
        '''
        LangChain ëª¨ë¸ì— ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì…ë ¥ëœ ë©”ì‹œì§€ ëª©ë¡ì„ ë³€í™˜í•©ë‹ˆë‹¤.

        Args:
            chat_prompts_raw (List[Union[dict, BaseMessage]]): 
                'role'ê³¼ 'content'ë¥¼ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬ í˜•íƒœ ë˜ëŠ” 
                LangChainì˜ ë©”ì‹œì§€ ê°ì²´(BaseMessage í•˜ìœ„ í´ë˜ìŠ¤)ë¡œ êµ¬ì„±ëœ ë¦¬ìŠ¤íŠ¸.

        Returns:
            List[BaseMessage]: 
                SystemMessage, HumanMessage, AIMessage ë“± LangChainì—ì„œ ì‚¬ìš©í•˜ëŠ” ë©”ì‹œì§€ ê°ì²´ ë¦¬ìŠ¤íŠ¸.

        Raises:
            ValueError: 
                ì•Œ ìˆ˜ ì—†ëŠ” ì—­í• (role)ì´ ìˆëŠ” ê²½ìš° ë˜ëŠ” ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì…ì´ í¬í•¨ëœ ê²½ìš° ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤.
        '''
        converted = []
        for item in chat_prompts_raw:
            if isinstance(item, dict):
                role = item.get("role")
                content = item.get("content", "")
                if role == "system":
                    converted.append(SystemMessage(content=content))
                elif role == "user":
                    converted.append(HumanMessage(content=content))
                elif role == "assistant":
                    converted.append(AIMessage(content=content))
                else:
                    raise ValueError(f"Unknown role: {role}")
            elif isinstance(item, BaseMessage):
                converted.append(item)
            else:
                raise ValueError(f"Unsupported chat_prompt type: {type(item)}")
        return converted

    # Define the node that calls the model
    async def call_model_node(self,
        state: CodeChatState,
        config: RunnableConfig,
        writer: StreamWriter
    ):
        chat_history = self.format_chat_history(state=state)
        user_query = self.get_last_user_message(state=state)
        
        # chat_type:str # 01: ê²€ìƒ‰ì–´ ì°¾ê¸°, 02: ì§ˆë¬¸í•˜ê¸°, 03: í›„ì†ì§ˆë¬¸í•˜ê¸°, 04: ì œëª©ì§“ê¸°, 05: íƒœê·¸ ì¶”ì¶œ
        chat_type = state.get('chat_type', '02')

        chat_prompts = []
        
        if (chat_type == "01") :
            chat_prompts = self.langfuse.get_prompt("AXLR_UI_CHAT_CODE_01").compile(
                current_date = datetime.now().strftime("%Y-%m-%d"), # í˜„ì¬ì¼ìë¥¼ yyyy-mm-dd í˜•ì‹ìœ¼ë¡œ í¬ë§·
                chat_history = chat_history
            )
        elif (chat_type == "02") :
            chat_prompts = self.langfuse.get_prompt("AXLR_UI_CHAT_CODE_02").compile(
                context = state.get("context", ""),
                user_query = user_query
            )
        elif (chat_type == "03") :
            chat_prompts = self.langfuse.get_prompt("AXLR_UI_CHAT_CODE_03").compile(
                chat_history = chat_history
            )
        elif (chat_type == "04") :
            chat_prompts = self.langfuse.get_prompt("AXLR_UI_CHAT_CODE_04").compile(
                chat_history = chat_history
            )
        elif (chat_type == "05") :
            chat_prompts = self.langfuse.get_prompt("AXLR_UI_CHAT_CODE_05").compile(
                chat_history = chat_history
            )
        else :
            chat_prompts = user_query
            pass
            
        # system ë©”ì„¸ì§€ ë’¤ì— ì±„íŒ…ì´ë ¥ ì¶”ê°€
        if chat_type in ("02") and isinstance(chat_prompts, list):
            messages = (state.get("messages") or [])[:-1]
            messages = messages[-3:]  # ìµœê·¼ 3ê°œë§Œ ìœ ì§€
            
            last_system_idx = -1
            for idx, msg in enumerate(chat_prompts):
                if isinstance(msg, dict) and msg.get("role") == "system":
                    last_system_idx = idx
            if last_system_idx >= 0:
                if messages:
                    chat_prompts = chat_prompts[:last_system_idx+1] + messages + chat_prompts[last_system_idx+1:]
            

        # LangChain ëª¨ë¸ì— ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì…ë ¥ëœ ë©”ì‹œì§€ ëª©ë¡ì„ ë³€í™˜
        chat_prompts = self.convert_to_messages(chat_prompts)
            
        # Stream ë°©ì‹
        tokens = []
        async for chunk in self.model.astream(chat_prompts, config):
            writer(chunk)
            tokens.append(str(chunk.content))
        state['response'] = "".join(tokens)

        # result = await self.model.ainvoke(chat_prompts)  # ë™ê¸° í˜¸ì¶œë¡œ ë³€ê²½
        # state["response"] = result.content
        
        return state

    # Define the conditional edge that determines whether to continue or not
    def should_continue(self, state: CodeChatState):
        messages = state["messages"]
        last_message = messages[-1]
        # If there is no function call, then we finish
        if not last_message.tool_calls:
            return "end"
        # Otherwise if there is, we continue
        else:
            return "continue"


    def clean_response_node(self, state: CodeChatState) -> CodeChatState:
        # chat_typeì´ 02ì¸ ê²½ìš°ë§Œ ì•„ë˜ ë¡œì§ì„ íƒ€ë„ë¡
        if state.get("chat_type") == "02":
            # Remove <source ...>íƒœê·¸ì™€ ê·¸ ë’¤ì— ë¶™ì€ 'ëŠ”', 'ì€', 'ì´', 'ê°€' ì¡°ì‚¬ê¹Œì§€ í•¨ê»˜ ì œê±°
            cleaned = re.sub(r"<source[^>]*>[ \t]*(?:ëŠ”|ì€|ì´|ê°€)?", "", state["response"])
            cleaned = re.sub(r"</source>", "", cleaned)
            state["response"] = cleaned
        return state


    # 1. file ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ì„ ê²½ìš°
    # 2. LLM 

    async def search_vector_datas_node(self, state:CodeChatState) -> CodeChatState:

        index_name = "code_assist" # ì¶”í›„ ì…ë ¥ê°’ì„ ë°›ì„ ìˆ˜ ìˆë„ë¡ ë³€ê²½
        search_text = self.get_last_user_message(state)

        # ì²´í¬
        if not search_text:
            return state
        
        # embedding-friendlyí•œ queryë¡œ ì¬ì‘ì„±í•œë‹¤.
        query_rewriting_prompt = self.langfuse.get_prompt("AXLR_UI_QUERY_REWRITE_CODE_SEARCH").compile(
            user_input = search_text
        )

        result = self.model.invoke(query_rewriting_prompt)

        try:
            raw_content = result.content.strip()

            # Markdown ì½”ë“œë¸”ëŸ­ ì œê±° ì²˜ë¦¬
            if raw_content.startswith("```"):
                lines = raw_content.splitlines()
                # ì²« ì¤„ì´ ```json ë˜ëŠ” ``` ì´ê³  ë§ˆì§€ë§‰ ì¤„ì´ ```ì´ë©´ ì œê±°
                if len(lines) >= 3 and lines[0].strip().startswith("```") and lines[-1].strip() == "```":
                    raw_content = "\n".join(lines[1:-1])

            # logger.info(f"### raw_content = {raw_content}")
            result_json = json.loads(raw_content)
            # logger.info(f"### result_json = {result_json}")
            
            rewritten_ko = result_json.get("rewritten_ko")
            rewritten_en = result_json.get("rewritten_en")
        except Exception as e:
            logger.info(f"### JSON íŒŒì‹± ì‹¤íŒ¨! ì—ëŸ¬: {e}\nê²°ê³¼:\n{result.content}")
            rewritten_ko = rewritten_en = None

        vector_store = get_vector_store(collection_name=index_name)
        
        search_ko_results, search_en_results = [], []
        if rewritten_ko:
            search_ko_results = vector_store.similarity_search_with_score(query=rewritten_ko, k=5)
        else:
            logger.info("### ë²¡í„° ê²€ìƒ‰ ìƒëµ: rewritten_koê°€ None")

        if rewritten_en:
            search_en_results = vector_store.similarity_search_with_score(query=rewritten_en, k=5)
        else:
            logger.info("### ë²¡í„° ê²€ìƒ‰ ìƒëµ: rewritten_enê°€ None")
        
        # ë¡œê·¸ì¶œë ¥
        # TODO: logging ëª¨ë“ˆë¡œ ì •í•´ì§„ ë””ë ‰í† ë¦¬ì— ì €ì¥í•˜ë„ë¡ í•œë‹¤.
        
        context_datas = state.get("context_datas", [])

        # 2ê°œì˜ ê²°ê³¼ê°’ì„ ë¨¸ì§€
        merged_results = search_ko_results + search_en_results
        
        # ì¤‘ë³µ ì œê±°: id ê¸°ì¤€ìœ¼ë¡œ dict êµ¬ì„± í›„ ë‹¤ì‹œ ë¦¬ìŠ¤íŠ¸ë¡œ
        unique_results_dict = {}
        for result in merged_results:
            result_id = result.get("id")
            if result_id and result_id not in unique_results_dict:
                unique_results_dict[result_id] = result
        merged_results = list(unique_results_dict.values())
        
        # logger.info(f"\n\n>>>>>>>>>>>>>>>>>>>>> merged_results = {merged_results}")
        
        for i, vector_data in enumerate(merged_results, start=1):
            metadata = vector_data.get("metadata", {})
            context_datas.append({
                "id": vector_data["id"],
                "seq": i,
                "type": "vectordb",
                "doc_id": vector_data.get("doc_id", ""),
                "doc_name": metadata.get("name", i),
                "chunked_data_id": metadata.get("chunked_data_id", ""),
                "name": metadata.get("doc_name", "vector"),
                "content": vector_data.get("content", "")
            })
        state["context_datas"] = context_datas

        # webuiì— ì¸ìš©ì •ë³´ë¥¼ ë³´ì—¬ì£¼ê¸° ìœ„í•˜ì—¬ ì €ì¥í•œë‹¤
        if merged_results:
            store_vector_sources(state.get("metadata"), context_datas=context_datas)
        
        return state


    def check_need_vector_search_node(self, state: CodeChatState) -> CodeChatState:
        metadata = state.get("metadata")
        if not isinstance(metadata, dict):
            metadata = {}
        features = metadata.get("features")
        if not isinstance(features, dict):
            features = {}
        
        # metadata.features.is_vectordb ì˜ ê°’ì´ Falseì´ë©´ íŒ¨ìŠ¤
        is_vectordb = features.get("is_vectordb", False)
        if is_vectordb in (False, "False"):
            state["is_vector_search"] = "no"
            return state
            
        # chat_typeì´ 02ì¼ ê²½ìš°ë§Œ í•„ìš”í•¨
        if state.get("chat_type") != "02":
            state["is_vector_search"] = "no"
            return state
        
        # ê¸°ì¡´ì— íŒŒì¼ë¡œ ë“¤ì–´ì˜¨ ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ íŒ¨ìŠ¤
        if self.check_added_file_context(state["context_datas"]):
            state["is_vector_search"] = "no"
            return state
        
        query = self.get_last_user_message(state)
        prompt = f"""
Determine whether a document search in the vector database is necessary for the following question.
Respond with â€˜yesâ€™ if needed, â€˜noâ€™ otherwise.
- The vector database contains source code of a securities trading system.

Question:
{query}
"""
        result = self.model.invoke([HumanMessage(content=prompt)])
        answer = result.content.strip().lower()
        
        decision = "yes" if "yes" in answer else "no"
        state["is_vector_search"] = decision
            
        return state

    def route_based_on_vector_need(self, state: CodeChatState) -> str:
        return state.get("is_vector_search", "no")


    def get_chain(self, thread_id: str = str(uuid.uuid4()), chat_type:str = "" , checkpointer = None):
        # tools = [self.get_weather]
        # self.model = self.model.bind_tools(tools)
        
        # TODO: langgraph 0.4.5 ì—…ë°ì´íŠ¸ ì´í›„ name ì…‹íŒ…í•´ì„œ trace name êµ¬ë¶„
        # graph = StateGraph(CodeChatState, name=f"AXLR_UI_CHAT_CODE_{chat_type}")
        graph = StateGraph(CodeChatState)
        
        graph.add_node("pre_process", self.pre_process_node)
        graph.add_node("check_need_vector_search", self.check_need_vector_search_node)
        graph.add_node("search_vector_datas", self.search_vector_datas_node)
        graph.add_node("merge_context_datas", self.merge_context_datas_node)
        graph.add_node("call_model", self.call_model_node)
        graph.add_node("clean_response", self.clean_response_node)

        graph.set_entry_point("pre_process")
        graph.add_edge("pre_process", "check_need_vector_search")
        graph.add_conditional_edges(
            "check_need_vector_search",
            self.route_based_on_vector_need,
            {
                "yes": "search_vector_datas",
                "no": "merge_context_datas"
            }
        )
        graph.add_edge("search_vector_datas", "merge_context_datas")
        graph.add_edge("merge_context_datas", "call_model")
        graph.add_edge("call_model", "clean_response")
        graph.add_edge("clean_response", END)
        # graph.add_edge("tools", "call_model")

        return graph.compile(checkpointer=checkpointer), thread_id

def store_vector_sources(metadata, context_datas):
    # metadata = state.get("metadata")
    chat_type = metadata.get("chat_type") if metadata else None
    user_id = metadata.get("user_id") if metadata else None
    chat_id = metadata.get("chat_id") if metadata else None
    message_id = metadata.get("message_id") if metadata else None
    
    logger.info(f"### [ë²¡í„° ì†ŒìŠ¤ ì €ì¥] user_id: {user_id}, chat_id: {chat_id}, message_id: {message_id}, chat_type: {chat_type}")
    
    if user_id and chat_id and message_id:
        # make_source_item now returns a list of sources
        sources = make_source_item(user_id=user_id, context_datas=context_datas)

        with get_axlrui_session() as session:
            # ì¡°íšŒ
            chat_row = session.query(Chat).filter(Chat.id == chat_id).first()
            if chat_row and chat_row.chat:
                try:
                    chat_data = chat_row.chat

                    # contentê°€ ë¬¸ìì—´ì¸ì§€ í™•ì¸í•˜ê³ , ì¸ì½”ë”© ë¬¸ì œ ì˜ˆë°©ì„ ìœ„í•œ ë³´ì •
                    if isinstance(chat_data, str):
                        chat_data = json.loads(chat_data)
                    if isinstance(chat_data, bytes):
                        chat_data = json.loads(chat_data.decode("utf-8", errors="replace"))
                    elif not isinstance(chat_data, dict):
                        chat_data = {}

                    # ë©”ì‹œì§€ì— ì†ŒìŠ¤ ì¶”ê°€
                    _history_messages = chat_data.get("history", {}).get("messages", {})
                    _history_msg = _history_messages.get(message_id)

                    if _history_msg:
                        _history_msg.setdefault("sources", []).extend(sources)

                    if message_id and isinstance(chat_data, dict):
                        # ë©”ì‹œì§€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
                        _messages = chat_data.get("messages", [])
                        for msg in _messages:
                            if msg.get("id") == message_id:
                                # sourcesì— ì†ŒìŠ¤ ì¶”ê°€
                                msg.setdefault("sources", [])
                                msg["sources"].extend(sources)

                                # íŒŒì¼ëª©ë¡ì— ì†ŒìŠ¤ ì¶”ê°€
                                chat_data.setdefault("files", []).extend([s.get("source") for s in sources])

                                # ì—…ë°ì´íŠ¸ëœ chat_data ë‚´ìš©ì„ DB update
                                chat_row.chat = chat_data
                                flag_modified(chat_row, "chat")  # ë³€ê²½ ê°ì§€ ê°•ì œ
                                session.flush()
                                session.commit()
                                break

                    return chat_data
                except Exception as e:
                    # ë¡œê¹…ì„ ì¶”ê°€í•˜ê³  None ë°˜í™˜
                    logger.info(f"Error parsing file data: {e}")
                    return None

def make_source_item(user_id: str, context_datas: list) -> list:
    from collections import defaultdict
    created_at = int(time.time())
    grouped = defaultdict(list)
    for item in context_datas:
        doc_id = item.get("doc_id")
        if doc_id:
            grouped[doc_id].append(item)

    sources = []

    for doc_id, items in grouped.items():
        resrc_org_id = doc_id
        resrc_name = items[0].get("doc_name", f"vector_{doc_id}")
        content_list = [item.get("content", "") for item in items]
        content_joined = "\n".join(item.get("content", "") for item in items)
        # logger.info(f">>>>>>>>>>>>>>>>>>> content_joined = {content_joined}")
        source = {
            "source": {
                "type": "milvus",
                "file": {
                    "id": resrc_org_id,
                    "user_id": user_id,
                    "hash": "",
                    "filename": resrc_name,
                    "data": {
                        "content": content_joined
                    },
                    "meta": {
                        "name": resrc_name,
                        "content_type": "text/plain",
                        "size": 0,
                        "data": {},
                        "collection_name": ""
                    },
                    "created_at": created_at,
                    "updated_at": created_at
                },
                "id": resrc_org_id,
                "url": f"/api/v1/files/{resrc_org_id}",
                "name": resrc_name,
                "collection_name": "",
                "status": "retrieve",
                "size": 0,
                "error": "",
                "itemId": ""
            },
            "document": content_list,
            "metadata": [
                {
                    "created_by": user_id,
                    "embedding_config": {"engine": "", "model": "sentence-transformers/all-MiniLM-L6-v2"},
                    "file_id": resrc_org_id,
                    "hash": "",
                    "name": resrc_name,
                    "source": resrc_name,
                    "start_index": 0
                }
                for _ in items
            ],
            "distances": [1.0 for _ in items]
        }

        sources.append(source)

    return sources

if __name__ == "__main__":
    """
    Langfuse í”„ë¡¬í”„íŠ¸ë¥¼ ê°€ì ¸ì™€ í˜„ì¬ì¼ìì™€ chat_historyë¥¼ ì‚¬ìš©í•´ promptë¥¼ ì»´íŒŒì¼í•©ë‹ˆë‹¤.
    """
    test = Langfuse().get_prompt("AXLR_UI_CHAT_CODE_01").compile(
        current_date=datetime.now().strftime("%Y-%m-%d"),
        chat_history="chat_history"
    )
    
    logger.info(test)

    # from langchain_core.messages import HumanMessage
    
    # graph = await CodeChatAgent.create(index_name="cg_code_assist")

    # config = {"configurable": {"thread_id": "2"}}
    # input_message = HumanMessage(content="ì•ˆë…•! ë‚´ ì´ë¦„ì€ í™ê¸¸ë™ì´ì•¼")
    # for event in graph.stream({"messages": [input_message]}, config, stream_mode="values"):
    #     event["messages"][-1].pretty_print()


    # input_message = HumanMessage(content="ë‚´ ì´ë¦„ì´ ë­ì•¼?")
    # for event in graph.stream({"messages": [input_message]}, config, stream_mode="values"):
    #     event["messages"][-1].pretty_print()
