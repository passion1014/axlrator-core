{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancing RAG with Contextual Retrieval\n",
    "\n",
    "> Note: For more background information on Contextual Retrieval, including additional performance evaluations on various datasets, we recommend reading our accompanying  [blog post](https://www.anthropic.com/news/contextual-retrieval).\n",
    "\n",
    "Retrieval Augmented Generation (RAG) enables Claude to leverage your internal knowledge bases, codebases, or any other corpus of documents when providing a response. Enterprises are increasingly building RAG applications to improve workflows in customer support, Q&A over internal company documents, financial & legal analysis, code generation, and much more.\n",
    "\n",
    "In a [separate guide](https://github.com/anthropics/anthropic-cookbook/blob/main/skills/retrieval_augmented_generation/guide.ipynb), we walked through setting up a basic retrieval system, demonstrated how to evaluate its performance, and then outlined a few techniques to improve performance. In this guide, we present a technique for improving retrieval performance: Contextual Embeddings.\n",
    "\n",
    "In traditional RAG, documents are typically split into smaller chunks for efficient retrieval. While this approach works well for many applications, it can lead to problems when individual chunks lack sufficient context. Contextual Embeddings solve this problem by adding relevant context to each chunk before embedding. This method improves the quality of each embedded chunk, allowing for more accurate retrieval and thus better overall performance. Averaged across all data sources we tested, Contextual Embeddings reduced the top-20-chunk retrieval failure rate by 35%.\n",
    "\n",
    "The same chunk-specific context can also be used with BM25 search to further improve retrieval performance. We introduce this technique in the “Contextual BM25” section.\n",
    "\n",
    "In this guide, we'll demonstrate how to build and optimize a Contextual Retrieval system using a dataset of 9 codebases as our knowledge base. We'll walk through:\n",
    "\n",
    "1) Setting up a basic retrieval pipeline to establish a baseline for performance.\n",
    "\n",
    "2) Contextual Embeddings: what it is, why it works, and how prompt caching makes it practical for production use cases.\n",
    "\n",
    "3) Implementing Contextual Embeddings and demonstrating performance improvements.\n",
    "\n",
    "4) Contextual BM25: improving performance with *contextual* BM25 hybrid search.\n",
    "\n",
    "5) Improving performance with reranking,\n",
    "\n",
    "### Evaluation Metrics & Dataset:\n",
    "\n",
    "We use a pre-chunked dataset of 9 codebases - all of which have been chunked according to a basic character splitting mechanism. Our evaluation dataset contains 248 queries - each of which contains a 'golden chunk.' We'll use a metric called Pass@k to evaluate performance. Pass@k checks whether or not the 'golden document' was present in the first k documents retrieved for each query. Contextual Embeddings in this case helped us to improve Pass@10 performance from ~87% --> ~95%.\n",
    "\n",
    "You can find the code files and their chunks in `data/codebase_chunks.json` and the evaluation dataset in `data/evaluation_set.jsonl`\n",
    "\n",
    "#### Additional Notes:\n",
    "\n",
    "Prompt caching is helpful in managing costs when using this retrieval method. This feature is currently available on Anthropic's 1P API, and is coming soon to our 3P partner environments in AWS Bedrock and GCP Vertex. We know that many of our customers leverage AWS Knowledge Bases and GCP Vertex AI APIs when building RAG solutions, and this method can be used on either platform with a bit of customization. Consider reaching out to Anthropic or your AWS/GCP account team for guidance on this!\n",
    "\n",
    "To make it easier to use this method on Bedrock, the AWS team has provided us with code that you can use to implement a Lambda function that adds context to each document. If you deploy this Lambda function, you can select it as a custom chunking option when configuring a [Bedrock Knowledge Base](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-create.html). You can find this code in `contextual-rag-lambda-function`. The main lambda function code is in `lambda_function.py`.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1) Setup\n",
    "\n",
    "2) Basic RAG\n",
    "\n",
    "3) Contextual Embeddings\n",
    "\n",
    "4) Contextual BM25\n",
    "\n",
    "5) Reranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll need a few libraries, including:\n",
    "\n",
    "1) `anthropic` - to interact with Claude\n",
    "\n",
    "2) `voyageai` - to generate high quality embeddings\n",
    "\n",
    "3) `cohere` - for reranking\n",
    "\n",
    "4) `elasticsearch` for performant BM25 search\n",
    "\n",
    "3) `pandas`, `numpy`, `matplotlib`, and `scikit-learn` for data manipulation and visualization\n",
    "\n",
    "\n",
    "You'll also need API keys from [Anthropic](https://www.anthropic.com/), [Voyage AI](https://www.voyageai.com/), and [Cohere](https://cohere.com/rerank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting anthropic\n",
      "  Obtaining dependency information for anthropic from https://files.pythonhosted.org/packages/4c/4f/2f71e9f337f2039b829f277978a5fa12c69f873b7e2934ee80bda2d13020/anthropic-0.35.0-py3-none-any.whl.metadata\n",
      "  Downloading anthropic-0.35.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from anthropic) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from anthropic) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from anthropic) (0.27.0)\n",
      "Collecting jiter<1,>=0.4.0 (from anthropic)\n",
      "  Obtaining dependency information for jiter<1,>=0.4.0 from https://files.pythonhosted.org/packages/91/35/85ef9eaef7dec14f28dd9b8a2116c07075bb2731a405b650a55fda4c74d7/jiter-0.6.1-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading jiter-0.6.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from anthropic) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from anthropic) (1.2.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from anthropic) (0.15.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from anthropic) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->anthropic) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->anthropic) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->anthropic) (2.20.1)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from tokenizers>=0.13.0->anthropic) (0.22.2)\n",
      "Requirement already satisfied: filelock in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (6.0.1)\n",
      "Requirement already satisfied: requests in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (4.66.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (1.26.18)\n",
      "Downloading anthropic-0.35.0-py3-none-any.whl (894 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m894.0/894.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.6.1-cp311-cp311-macosx_11_0_arm64.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: jiter, anthropic\n",
      "Successfully installed anthropic-0.35.0 jiter-0.6.1\n",
      "Collecting voyageai\n",
      "  Obtaining dependency information for voyageai from https://files.pythonhosted.org/packages/bf/7c/43fb4689fe287eceb701f389863aab35211835d63bbb9a798cfefa80d7de/voyageai-0.2.3-py3-none-any.whl.metadata\n",
      "  Downloading voyageai-0.2.3-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: aiohttp<4.0,>=3.5 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from voyageai) (3.8.5)\n",
      "Collecting aiolimiter<2.0.0,>=1.1.0 (from voyageai)\n",
      "  Obtaining dependency information for aiolimiter<2.0.0,>=1.1.0 from https://files.pythonhosted.org/packages/60/69/4b7dea755fafa10b248928da836a2cc8b5cff0762f363234e24218040f8e/aiolimiter-1.1.0-py3-none-any.whl.metadata\n",
      "  Using cached aiolimiter-1.1.0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: numpy>=1.11 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from voyageai) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0,>=2.20 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from voyageai) (2.31.0)\n",
      "Requirement already satisfied: tenacity>=8.0.1 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from voyageai) (8.2.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0,>=3.5->voyageai) (23.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0,>=3.5->voyageai) (3.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0,>=3.5->voyageai) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0,>=3.5->voyageai) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0,>=3.5->voyageai) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0,>=3.5->voyageai) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0,>=3.5->voyageai) (1.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from requests<3.0,>=2.20->voyageai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from requests<3.0,>=2.20->voyageai) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from requests<3.0,>=2.20->voyageai) (2024.2.2)\n",
      "Downloading voyageai-0.2.3-py3-none-any.whl (19 kB)\n",
      "Using cached aiolimiter-1.1.0-py3-none-any.whl (7.2 kB)\n",
      "Installing collected packages: aiolimiter, voyageai\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "graphrag 0.1.1 requires aiofiles<25.0.0,>=24.1.0, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiolimiter-1.1.0 voyageai-0.2.3\n",
      "Collecting cohere\n",
      "  Obtaining dependency information for cohere from https://files.pythonhosted.org/packages/5e/ac/e16f7bbdb370607e991270747ba85664c643057b06c76e5d7030acecd164/cohere-5.11.0-py3-none-any.whl.metadata\n",
      "  Downloading cohere-5.11.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting boto3<2.0.0,>=1.34.0 (from cohere)\n",
      "  Obtaining dependency information for boto3<2.0.0,>=1.34.0 from https://files.pythonhosted.org/packages/9c/e5/69df0f21c1de7208121c2c2baab06ff3c4aa51bf710798d13fb76e4c800d/boto3-1.35.35-py3-none-any.whl.metadata\n",
      "  Downloading boto3-1.35.35-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
      "  Obtaining dependency information for fastavro<2.0.0,>=1.9.4 from https://files.pythonhosted.org/packages/89/61/b8b18aebc01e5d5a77042f6d555fe091d3279242edd5639252c9fcb9a3b7/fastavro-1.9.7-cp311-cp311-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading fastavro-1.9.7-cp311-cp311-macosx_10_9_universal2.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: httpx>=0.21.2 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from cohere) (0.27.0)\n",
      "Collecting httpx-sse==0.4.0 (from cohere)\n",
      "  Obtaining dependency information for httpx-sse==0.4.0 from https://files.pythonhosted.org/packages/e1/9b/a181f281f65d776426002f330c31849b86b31fc9d848db62e16f03ff739f/httpx_sse-0.4.0-py3-none-any.whl.metadata\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting parameterized<0.10.0,>=0.9.0 (from cohere)\n",
      "  Obtaining dependency information for parameterized<0.10.0,>=0.9.0 from https://files.pythonhosted.org/packages/00/2f/804f58f0b856ab3bf21617cccf5b39206e6c4c94c2cd227bde125ea6105f/parameterized-0.9.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: pydantic>=1.9.2 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from cohere) (2.8.2)\n",
      "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from cohere) (2.20.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from cohere) (2.31.0)\n",
      "Collecting sagemaker<3.0.0,>=2.232.1 (from cohere)\n",
      "  Obtaining dependency information for sagemaker<3.0.0,>=2.232.1 from https://files.pythonhosted.org/packages/a7/79/0e994696f551ae78cbe0333e6b7ebe9e5e523e2d83a212b7394018ffd8f1/sagemaker-2.232.2-py3-none-any.whl.metadata\n",
      "  Downloading sagemaker-2.232.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from cohere) (0.15.2)\n",
      "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from cohere) (2.31.0.20240311)\n",
      "Requirement already satisfied: typing_extensions>=4.0.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from cohere) (4.12.2)\n",
      "Collecting botocore<1.36.0,>=1.35.35 (from boto3<2.0.0,>=1.34.0->cohere)\n",
      "  Obtaining dependency information for botocore<1.36.0,>=1.35.35 from https://files.pythonhosted.org/packages/d8/5e/2f5d571be31ada15003f35c4dadabce80a8c9c6aa941ab11e6bd3623af53/botocore-1.35.35-py3-none-any.whl.metadata\n",
      "  Downloading botocore-1.35.35-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from boto3<2.0.0,>=1.34.0->cohere) (0.10.0)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0.0,>=1.34.0->cohere)\n",
      "  Obtaining dependency information for s3transfer<0.11.0,>=0.10.0 from https://files.pythonhosted.org/packages/3c/4a/b221409913760d26cf4498b7b1741d510c82d3ad38381984a3ddc135ec66/s3transfer-0.10.2-py3-none-any.whl.metadata\n",
      "  Downloading s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: anyio in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from httpx>=0.21.2->cohere) (4.6.0)\n",
      "Requirement already satisfied: certifi in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from httpx>=0.21.2->cohere) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from httpx>=0.21.2->cohere) (1.0.4)\n",
      "Requirement already satisfied: idna in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from httpx>=0.21.2->cohere) (3.10)\n",
      "Requirement already satisfied: sniffio in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from httpx>=0.21.2->cohere) (1.2.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->cohere) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->cohere) (1.26.18)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from sagemaker<3.0.0,>=2.232.1->cohere) (23.2.0)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from sagemaker<3.0.0,>=2.232.1->cohere) (2.2.1)\n",
      "Collecting docker (from sagemaker<3.0.0,>=2.232.1->cohere)\n",
      "  Obtaining dependency information for docker from https://files.pythonhosted.org/packages/e3/26/57c6fb270950d476074c087527a558ccb6f4436657314bfb6cdf484114c4/docker-7.1.0-py3-none-any.whl.metadata\n",
      "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting google-pasta (from sagemaker<3.0.0,>=2.232.1->cohere)\n",
      "  Obtaining dependency information for google-pasta from https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl.metadata\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from sagemaker<3.0.0,>=2.232.1->cohere) (6.0.0)\n",
      "Requirement already satisfied: jsonschema in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from sagemaker<3.0.0,>=2.232.1->cohere) (4.22.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from sagemaker<3.0.0,>=2.232.1->cohere) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from sagemaker<3.0.0,>=2.232.1->cohere) (23.2)\n",
      "Requirement already satisfied: pandas in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from sagemaker<3.0.0,>=2.232.1->cohere) (2.2.2)\n",
      "Collecting pathos (from sagemaker<3.0.0,>=2.232.1->cohere)\n",
      "  Obtaining dependency information for pathos from https://files.pythonhosted.org/packages/77/f6/a459cf58ff6b2d1c0a1961ee7084f4bb549d50e288f064e7e7be5ae3a7ab/pathos-0.3.3-py3-none-any.whl.metadata\n",
      "  Downloading pathos-0.3.3-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: platformdirs in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from sagemaker<3.0.0,>=2.232.1->cohere) (3.10.0)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from sagemaker<3.0.0,>=2.232.1->cohere) (4.24.4)\n",
      "Requirement already satisfied: psutil in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from sagemaker<3.0.0,>=2.232.1->cohere) (5.9.0)\n",
      "Requirement already satisfied: pyyaml~=6.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from sagemaker<3.0.0,>=2.232.1->cohere) (6.0.1)\n",
      "Collecting sagemaker-core<2.0.0,>=1.0.0 (from sagemaker<3.0.0,>=2.232.1->cohere)\n",
      "  Obtaining dependency information for sagemaker-core<2.0.0,>=1.0.0 from https://files.pythonhosted.org/packages/18/af/6bec7c84b1a4dd4160711bf9dd80fae87fe421892b3416971e78cac6f903/sagemaker_core-1.0.10-py3-none-any.whl.metadata\n",
      "  Downloading sagemaker_core-1.0.10-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting sagemaker-mlflow (from sagemaker<3.0.0,>=2.232.1->cohere)\n",
      "  Obtaining dependency information for sagemaker-mlflow from https://files.pythonhosted.org/packages/5e/54/e2dd3cf3e289e480600649b1e3f573ca54c8df1db1ac78cc55a24279b924/sagemaker_mlflow-0.1.0-py3-none-any.whl.metadata\n",
      "  Downloading sagemaker_mlflow-0.1.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting schema (from sagemaker<3.0.0,>=2.232.1->cohere)\n",
      "  Obtaining dependency information for schema from https://files.pythonhosted.org/packages/ad/1b/81855a88c6db2b114d5b2e9f96339190d5ee4d1b981d217fa32127bb00e0/schema-0.7.7-py2.py3-none-any.whl.metadata\n",
      "  Downloading schema-0.7.7-py2.py3-none-any.whl.metadata (34 kB)\n",
      "Collecting smdebug-rulesconfig==1.0.1 (from sagemaker<3.0.0,>=2.232.1->cohere)\n",
      "  Obtaining dependency information for smdebug-rulesconfig==1.0.1 from https://files.pythonhosted.org/packages/26/a1/45a13a05198bbe9527bab2c5e5daa8bd02678aa825eec14783e767bfa7d1/smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl.metadata (943 bytes)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from sagemaker<3.0.0,>=2.232.1->cohere) (1.7.0)\n",
      "Requirement already satisfied: tqdm in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from sagemaker<3.0.0,>=2.232.1->cohere) (4.66.2)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from tokenizers<1,>=0.15->cohere) (0.22.2)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.0.0->cohere)\n",
      "  Obtaining dependency information for urllib3<3,>=1.21.1 from https://files.pythonhosted.org/packages/ce/d9/5f4c13cecde62396b0d3fe530a50ccea91e7dfc1ccf0e09c228841bb5ba8/urllib3-2.2.3-py3-none-any.whl.metadata\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from botocore<1.36.0,>=1.35.35->boto3<2.0.0,>=1.34.0->cohere) (2.8.2)\n",
      "Requirement already satisfied: filelock in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2024.3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker<3.0.0,>=2.232.1->cohere) (3.11.0)\n",
      "Collecting platformdirs (from sagemaker<3.0.0,>=2.232.1->cohere)\n",
      "  Obtaining dependency information for platformdirs from https://files.pythonhosted.org/packages/3c/a6/bc1012356d8ece4d66dd75c4b9fc6c1f6650ddd5991e421177d9f8f671be/platformdirs-4.3.6-py3-none-any.whl.metadata\n",
      "  Downloading platformdirs-4.3.6-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.0.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere) (13.7.1)\n",
      "Collecting mock<5.0,>4.0 (from sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere)\n",
      "  Obtaining dependency information for mock<5.0,>4.0 from https://files.pythonhosted.org/packages/5c/03/b7e605db4a57c0f6fba744b11ef3ddf4ddebcada35022927a2b5fc623fdf/mock-4.0.3-py3-none-any.whl.metadata\n",
      "  Downloading mock-4.0.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from jsonschema->sagemaker<3.0.0,>=2.232.1->cohere) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from jsonschema->sagemaker<3.0.0,>=2.232.1->cohere) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from jsonschema->sagemaker<3.0.0,>=2.232.1->cohere) (0.18.1)\n",
      "Requirement already satisfied: six in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from google-pasta->sagemaker<3.0.0,>=2.232.1->cohere) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from pandas->sagemaker<3.0.0,>=2.232.1->cohere) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from pandas->sagemaker<3.0.0,>=2.232.1->cohere) (2023.3)\n",
      "Collecting ppft>=1.7.6.9 (from pathos->sagemaker<3.0.0,>=2.232.1->cohere)\n",
      "  Obtaining dependency information for ppft>=1.7.6.9 from https://files.pythonhosted.org/packages/02/b3/45a04dabc39d93ad4836d99625e7c5350257b48e9ae2c5b701f3d5da6960/ppft-1.7.6.9-py3-none-any.whl.metadata\n",
      "  Downloading ppft-1.7.6.9-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting dill>=0.3.9 (from pathos->sagemaker<3.0.0,>=2.232.1->cohere)\n",
      "  Obtaining dependency information for dill>=0.3.9 from https://files.pythonhosted.org/packages/46/d1/e73b6ad76f0b1fb7f23c35c6d95dbc506a9c8804f43dda8cb5b0fa6331fd/dill-0.3.9-py3-none-any.whl.metadata\n",
      "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pox>=0.3.5 (from pathos->sagemaker<3.0.0,>=2.232.1->cohere)\n",
      "  Obtaining dependency information for pox>=0.3.5 from https://files.pythonhosted.org/packages/1d/4c/490d8f7825f38fa77bff188c568163f222d01f6c6d76f574429135edfc49/pox-0.3.5-py3-none-any.whl.metadata\n",
      "  Downloading pox-0.3.5-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting multiprocess>=0.70.17 (from pathos->sagemaker<3.0.0,>=2.232.1->cohere)\n",
      "  Obtaining dependency information for multiprocess>=0.70.17 from https://files.pythonhosted.org/packages/b2/07/8cbb75d6cfbe8712d8f7f6a5615f083c6e710ab916b748fbb20373ddb142/multiprocess-0.70.17-py311-none-any.whl.metadata\n",
      "  Downloading multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting mlflow>=2.8 (from sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
      "  Obtaining dependency information for mlflow>=2.8 from https://files.pythonhosted.org/packages/e5/0f/a1fbf8aa0040244a51464ca97fd554e1a8728727caf7e312f4b9b2f8b1fa/mlflow-2.16.2-py3-none-any.whl.metadata\n",
      "  Downloading mlflow-2.16.2-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting mlflow-skinny==2.16.2 (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
      "  Obtaining dependency information for mlflow-skinny==2.16.2 from https://files.pythonhosted.org/packages/46/ba/2300ec30b6425507fface15601a6e20320ace45586d9a66339ac19644381/mlflow_skinny-2.16.2-py3-none-any.whl.metadata\n",
      "  Downloading mlflow_skinny-2.16.2-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: Flask<4 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (2.2.2)\n",
      "Collecting alembic!=1.10.0,<2 (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
      "  Obtaining dependency information for alembic!=1.10.0,<2 from https://files.pythonhosted.org/packages/c2/12/58f4f11385fddafef5d6f7bfaaf2f42899c8da6b4f95c04b7c3b744851a8/alembic-1.13.3-py3-none-any.whl.metadata\n",
      "  Downloading alembic-1.13.3-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting graphene<4 (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
      "  Obtaining dependency information for graphene<4 from https://files.pythonhosted.org/packages/24/70/96f6027cdfc9bb89fc07627b615cb43fb1c443c93498412beaeaf157e9f1/graphene-3.3-py2.py3-none-any.whl.metadata\n",
      "  Downloading graphene-3.3-py2.py3-none-any.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (3.4.1)\n",
      "Requirement already satisfied: matplotlib<4 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (3.9.1)\n",
      "Requirement already satisfied: pyarrow<18,>=4.0.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (15.0.0)\n",
      "Requirement already satisfied: scikit-learn<2 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (1.5.1)\n",
      "Requirement already satisfied: scipy<2 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (1.12.0)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (1.4.39)\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (3.1.2)\n",
      "Collecting gunicorn<24 (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
      "  Obtaining dependency information for gunicorn<24 from https://files.pythonhosted.org/packages/cb/7d/6dac2a6e1eba33ee43f318edbed4ff29151a49b5d37f080aad1e6469bca4/gunicorn-23.0.0-py3-none-any.whl.metadata\n",
      "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (5.3.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (8.1.7)\n",
      "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
      "  Obtaining dependency information for databricks-sdk<1,>=0.20.0 from https://files.pythonhosted.org/packages/66/21/f1822aa66fa4041e884cd3750b392ebd7e7f46ea80591c939f25c1900b22/databricks_sdk-0.34.0-py3-none-any.whl.metadata\n",
      "  Downloading databricks_sdk-0.34.0-py3-none-any.whl.metadata (37 kB)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (3.1.43)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (1.23.0)\n",
      "Collecting sqlparse<1,>=0.4.0 (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
      "  Obtaining dependency information for sqlparse<1,>=0.4.0 from https://files.pythonhosted.org/packages/5d/a5/b2860373aa8de1e626b2bdfdd6df4355f0565b47e51f7d0c54fe70faf8fe/sqlparse-0.5.1-py3-none-any.whl.metadata\n",
      "  Downloading sqlparse-0.5.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere) (2.15.1)\n",
      "Collecting Mako (from alembic!=1.10.0,<2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
      "  Obtaining dependency information for Mako from https://files.pythonhosted.org/packages/03/62/70f5a0c2dd208f9f3f2f9afd103aec42ee4d9ad2401d78342f75e9b8da36/Mako-1.3.5-py3-none-any.whl.metadata\n",
      "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: Werkzeug>=2.2.2 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (2.2.3)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (2.0.1)\n",
      "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
      "  Obtaining dependency information for graphql-core<3.3,>=3.1 from https://files.pythonhosted.org/packages/d1/33/cc72c4c658c6316f188a60bc4e5a91cd4ceaaa8c3e7e691ac9297e4e72c7/graphql_core-3.2.4-py3-none-any.whl.metadata\n",
      "  Downloading graphql_core-3.2.4-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
      "  Obtaining dependency information for graphql-relay<3.3,>=3.1 from https://files.pythonhosted.org/packages/74/16/a4cf06adbc711bd364a73ce043b0b08d8fa5aae3df11b6ee4248bcdad2e0/graphql_relay-3.2.0-py3-none-any.whl.metadata\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting aniso8601<10,>=8 (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere)\n",
      "  Obtaining dependency information for aniso8601<10,>=8 from https://files.pythonhosted.org/packages/e3/04/e97c12dc034791d7b504860acfcdd2963fa21ae61eaca1c9d31245f812c3/aniso8601-9.0.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading aniso8601-9.0.1-py2.py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from Jinja2<4,>=2.11->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (2.1.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere) (0.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (1.4.4)\n",
      "Requirement already satisfied: pillow>=8 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (3.0.9)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from scikit-learn<2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from scikit-learn<2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (3.5.0)\n",
      "Requirement already satisfied: google-auth~=2.0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (2.27.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (4.0.11)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (1.2.14)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.44b0 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (0.44b0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (1.16.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (5.0.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere) (0.4.8)\n",
      "Downloading cohere-5.11.0-py3-none-any.whl (249 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.2/249.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading boto3-1.35.35-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastavro-1.9.7-cp311-cp311-macosx_10_9_universal2.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
      "Downloading sagemaker-2.232.2-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (20 kB)\n",
      "Downloading botocore-1.35.35-py3-none-any.whl (12.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sagemaker_core-1.0.10-py3-none-any.whl (388 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.4/388.4 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading platformdirs-4.3.6-py3-none-any.whl (18 kB)\n",
      "Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.3/126.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading pathos-0.3.3-py3-none-any.whl (82 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sagemaker_mlflow-0.1.0-py3-none-any.whl (24 kB)\n",
      "Downloading schema-0.7.7-py2.py3-none-any.whl (18 kB)\n",
      "Downloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mlflow-2.16.2-py3-none-any.whl (26.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading mlflow_skinny-2.16.2-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
      "Downloading multiprocess-0.70.17-py311-none-any.whl (144 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.3/144.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pox-0.3.5-py3-none-any.whl (29 kB)\n",
      "Downloading ppft-1.7.6.9-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading alembic-1.13.3-py3-none-any.whl (233 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.2/233.2 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading graphene-3.3-py2.py3-none-any.whl (128 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading databricks_sdk-0.34.0-py3-none-any.whl (565 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.6/565.6 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading graphql_core-3.2.4-py3-none-any.whl (203 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading sqlparse-0.5.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: schema, aniso8601, urllib3, sqlparse, smdebug-rulesconfig, ppft, pox, platformdirs, parameterized, mock, Mako, httpx-sse, gunicorn, graphql-core, google-pasta, fastavro, dill, multiprocess, graphql-relay, botocore, alembic, s3transfer, pathos, graphene, docker, databricks-sdk, mlflow-skinny, boto3, sagemaker-core, mlflow, sagemaker-mlflow, sagemaker, cohere\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.18\n",
      "    Uninstalling urllib3-1.26.18:\n",
      "      Successfully uninstalled urllib3-1.26.18\n",
      "  Attempting uninstall: platformdirs\n",
      "    Found existing installation: platformdirs 3.10.0\n",
      "    Uninstalling platformdirs-3.10.0:\n",
      "      Successfully uninstalled platformdirs-3.10.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.6\n",
      "    Uninstalling dill-0.3.6:\n",
      "      Successfully uninstalled dill-0.3.6\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.14\n",
      "    Uninstalling multiprocess-0.70.14:\n",
      "      Successfully uninstalled multiprocess-0.70.14\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.31.71\n",
      "    Uninstalling botocore-1.31.71:\n",
      "      Successfully uninstalled botocore-1.31.71\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.7.0\n",
      "    Uninstalling s3transfer-0.7.0:\n",
      "      Successfully uninstalled s3transfer-0.7.0\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.28.71\n",
      "    Uninstalling boto3-1.28.71:\n",
      "      Successfully uninstalled boto3-1.28.71\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.4.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.4.3 requires pyqtwebengine<5.16, which is not installed.\n",
      "datasets 2.12.0 requires dill<0.3.7,>=0.3.0, but you have dill 0.3.9 which is incompatible.\n",
      "ivy 0.0.6.2 requires urllib3<2.0, but you have urllib3 2.2.3 which is incompatible.\n",
      "unstructured 0.12.6 requires idna==3.6, but you have idna 3.10 which is incompatible.\n",
      "unstructured 0.12.6 requires joblib==1.3.2, but you have joblib 1.4.2 which is incompatible.\n",
      "unstructured 0.12.6 requires typing-extensions==4.9.0, but you have typing-extensions 4.12.2 which is incompatible.\n",
      "unstructured 0.12.6 requires urllib3==1.26.18, but you have urllib3 2.2.3 which is incompatible.\n",
      "python-lsp-black 1.2.1 requires black>=22.3.0, but you have black 0.0 which is incompatible.\n",
      "anaconda-cloud-auth 0.1.3 requires pydantic<2.0, but you have pydantic 2.8.2 which is incompatible.\n",
      "tts 0.22.0 requires pandas<2.0,>=1.4, but you have pandas 2.2.2 which is incompatible.\n",
      "langchain-experimental 0.0.55 requires langchain<0.2.0,>=0.1.13, but you have langchain 0.3.1 which is incompatible.\n",
      "langchain-experimental 0.0.55 requires langchain-core<0.2.0,>=0.1.33, but you have langchain-core 0.3.8 which is incompatible.\n",
      "aiobotocore 2.5.0 requires botocore<1.29.77,>=1.29.76, but you have botocore 1.35.35 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Mako-1.3.5 alembic-1.13.3 aniso8601-9.0.1 boto3-1.35.35 botocore-1.35.35 cohere-5.11.0 databricks-sdk-0.34.0 dill-0.3.9 docker-7.1.0 fastavro-1.9.7 google-pasta-0.2.0 graphene-3.3 graphql-core-3.2.4 graphql-relay-3.2.0 gunicorn-23.0.0 httpx-sse-0.4.0 mlflow-2.16.2 mlflow-skinny-2.16.2 mock-4.0.3 multiprocess-0.70.17 parameterized-0.9.0 pathos-0.3.3 platformdirs-4.3.6 pox-0.3.5 ppft-1.7.6.9 s3transfer-0.10.2 sagemaker-2.232.2 sagemaker-core-1.0.10 sagemaker-mlflow-0.1.0 schema-0.7.7 smdebug-rulesconfig-1.0.1 sqlparse-0.5.1 urllib3-2.2.3\n",
      "Collecting elasticsearch\n",
      "  Obtaining dependency information for elasticsearch from https://files.pythonhosted.org/packages/de/20/6f1d6977f68389116e40a0108a5bfd468f3a0cceabe90b522693834bb5ec/elasticsearch-8.15.1-py3-none-any.whl.metadata\n",
      "  Downloading elasticsearch-8.15.1-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting elastic-transport<9,>=8.13 (from elasticsearch)\n",
      "  Obtaining dependency information for elastic-transport<9,>=8.13 from https://files.pythonhosted.org/packages/1d/4a/df755b094170351b8199074fe8d0dfe6d639ab78a2e8893b556620ae982a/elastic_transport-8.15.0-py3-none-any.whl.metadata\n",
      "  Downloading elastic_transport-8.15.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from elastic-transport<9,>=8.13->elasticsearch) (2.2.3)\n",
      "Requirement already satisfied: certifi in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from elastic-transport<9,>=8.13->elasticsearch) (2024.2.2)\n",
      "Downloading elasticsearch-8.15.1-py3-none-any.whl (524 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.6/524.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading elastic_transport-8.15.0-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: elastic-transport, elasticsearch\n",
      "Successfully installed elastic-transport-8.15.0 elasticsearch-8.15.1\n",
      "Requirement already satisfied: pandas in /Users/passion1014/anaconda3/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/passion1014/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: numpy in /Users/passion1014/anaconda3/lib/python3.11/site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install anthropic\n",
    "!pip install voyageai\n",
    "!pip install cohere\n",
    "!pip install elasticsearch\n",
    "!pip install pandas\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['VOYAGE_API_KEY'] = \"pa-U_VipvgrWo3ce7-H6bq8o4oVnvU6s_EB80_tKeRyj9Y\"\n",
    "os.environ['ANTHROPIC_API_KEY'] = \"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\"\n",
    "os.environ['COHERE_API_KEY'] = \"dSOSV1mCtqeKwLRaR5qQthpLf8KXktzlKKUxwnYR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize a Vector DB Class\n",
    "\n",
    "In this example, we're using an in-memory vector DB, but for a production application, you may want to use a hosted solution. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import voyageai\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "class VectorDB:\n",
    "    def __init__(self, name: str, api_key = None):\n",
    "        if api_key is None:\n",
    "            api_key = os.getenv(\"VOYAGE_API_KEY\")\n",
    "        self.client = voyageai.Client(api_key=api_key)\n",
    "        self.name = name\n",
    "        self.embeddings = []\n",
    "        self.metadata = []\n",
    "        self.query_cache = {}\n",
    "        self.db_path = f\"./data/{name}/vector_db.pkl\"\n",
    "\n",
    "    def load_data(self, dataset: List[Dict[str, Any]]):\n",
    "        if self.embeddings and self.metadata:\n",
    "            print(\"Vector database is already loaded. Skipping data loading.\")\n",
    "            return\n",
    "        if os.path.exists(self.db_path):\n",
    "            print(\"Loading vector database from disk.\")\n",
    "            self.load_db()\n",
    "            return\n",
    "\n",
    "        texts_to_embed = []\n",
    "        metadata = []\n",
    "        total_chunks = sum(len(doc['chunks']) for doc in dataset)\n",
    "        \n",
    "        with tqdm(total=total_chunks, desc=\"Processing chunks\") as pbar:\n",
    "            for doc in dataset:\n",
    "                for chunk in doc['chunks']:\n",
    "                    texts_to_embed.append(chunk['content'])\n",
    "                    metadata.append({\n",
    "                        'doc_id': doc['doc_id'],\n",
    "                        'original_uuid': doc['original_uuid'],\n",
    "                        'chunk_id': chunk['chunk_id'],\n",
    "                        'original_index': chunk['original_index'],\n",
    "                        'content': chunk['content']\n",
    "                    })\n",
    "                    pbar.update(1)\n",
    "\n",
    "        self._embed_and_store(texts_to_embed, metadata)\n",
    "        self.save_db()\n",
    "        \n",
    "        print(f\"Vector database loaded and saved. Total chunks processed: {len(texts_to_embed)}\")\n",
    "\n",
    "    def _embed_and_store(self, texts: List[str], data: List[Dict[str, Any]]):\n",
    "        batch_size = 128\n",
    "        with tqdm(total=len(texts), desc=\"Embedding chunks\") as pbar:\n",
    "            result = []\n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                batch = texts[i : i + batch_size]\n",
    "                batch_result = self.client.embed(batch, model=\"voyage-2\").embeddings\n",
    "                result.extend(batch_result)\n",
    "                pbar.update(len(batch))\n",
    "        \n",
    "        self.embeddings = result\n",
    "        self.metadata = data\n",
    "\n",
    "    def search(self, query: str, k: int = 20) -> List[Dict[str, Any]]:\n",
    "        if query in self.query_cache:\n",
    "            query_embedding = self.query_cache[query]\n",
    "        else:\n",
    "            query_embedding = self.client.embed([query], model=\"voyage-2\").embeddings[0]\n",
    "            self.query_cache[query] = query_embedding\n",
    "\n",
    "        if not self.embeddings:\n",
    "            raise ValueError(\"No data loaded in the vector database.\")\n",
    "\n",
    "        similarities = np.dot(self.embeddings, query_embedding)\n",
    "        top_indices = np.argsort(similarities)[::-1][:k]\n",
    "        \n",
    "        top_results = []\n",
    "        for idx in top_indices:\n",
    "            result = {\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": float(similarities[idx]),\n",
    "            }\n",
    "            top_results.append(result)\n",
    "        \n",
    "        return top_results\n",
    "\n",
    "    def save_db(self):\n",
    "        data = {\n",
    "            \"embeddings\": self.embeddings,\n",
    "            \"metadata\": self.metadata,\n",
    "            \"query_cache\": json.dumps(self.query_cache),\n",
    "        }\n",
    "        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)\n",
    "        with open(self.db_path, \"wb\") as file:\n",
    "            pickle.dump(data, file)\n",
    "\n",
    "    def load_db(self):\n",
    "        if not os.path.exists(self.db_path):\n",
    "            raise ValueError(\"Vector database file not found. Use load_data to create a new database.\")\n",
    "        with open(self.db_path, \"rb\") as file:\n",
    "            data = pickle.load(file)\n",
    "        self.embeddings = data[\"embeddings\"]\n",
    "        self.metadata = data[\"metadata\"]\n",
    "        self.query_cache = json.loads(data[\"query_cache\"])\n",
    "\n",
    "    def validate_embedded_chunks(self):\n",
    "        unique_contents = set()\n",
    "        for meta in self.metadata:\n",
    "            unique_contents.add(meta['content'])\n",
    "    \n",
    "        print(f\"Validation results:\")\n",
    "        print(f\"Total embedded chunks: {len(self.metadata)}\")\n",
    "        print(f\"Unique embedded contents: {len(unique_contents)}\")\n",
    "    \n",
    "        if len(self.metadata) != len(unique_contents):\n",
    "            print(\"Warning: There may be duplicate chunks in the embedded data.\")\n",
    "        else:\n",
    "            print(\"All embedded chunks are unique.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your transformed dataset\n",
    "with open('data/codebase_chunks.json', 'r') as f:\n",
    "    transformed_dataset = json.load(f)\n",
    "\n",
    "# Initialize the VectorDB\n",
    "base_db = VectorDB(\"base_db\")\n",
    "\n",
    "# Load and process the data\n",
    "base_db.load_data(transformed_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RAG\n",
    "\n",
    "To get started, we'll set up a basic RAG pipeline using a bare bones approach. This is sometimes called 'Naive RAG' by many in the industry. A basic RAG pipeline includes the following 3 steps:\n",
    "\n",
    "1) Chunk documents by heading - containing only the content from each subheading\n",
    "\n",
    "2) Embed each document\n",
    "\n",
    "3) Use Cosine similarity to retrieve documents in order to answer query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict, Any, Callable, Union\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_jsonl(file_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load JSONL file and return a list of dictionaries.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        return [json.loads(line) for line in file]\n",
    "\n",
    "def evaluate_retrieval(queries: List[Dict[str, Any]], retrieval_function: Callable, db, k: int = 20) -> Dict[str, float]:\n",
    "    total_score = 0\n",
    "    total_queries = len(queries)\n",
    "    \n",
    "    for query_item in tqdm(queries, desc=\"Evaluating retrieval\"):\n",
    "        query = query_item['query']\n",
    "        golden_chunk_uuids = query_item['golden_chunk_uuids']\n",
    "        \n",
    "        # Find all golden chunk contents\n",
    "        golden_contents = []\n",
    "        for doc_uuid, chunk_index in golden_chunk_uuids:\n",
    "            golden_doc = next((doc for doc in query_item['golden_documents'] if doc['uuid'] == doc_uuid), None)\n",
    "            if not golden_doc:\n",
    "                print(f\"Warning: Golden document not found for UUID {doc_uuid}\")\n",
    "                continue\n",
    "            \n",
    "            golden_chunk = next((chunk for chunk in golden_doc['chunks'] if chunk['index'] == chunk_index), None)\n",
    "            if not golden_chunk:\n",
    "                print(f\"Warning: Golden chunk not found for index {chunk_index} in document {doc_uuid}\")\n",
    "                continue\n",
    "            \n",
    "            golden_contents.append(golden_chunk['content'].strip())\n",
    "        \n",
    "        if not golden_contents:\n",
    "            print(f\"Warning: No golden contents found for query: {query}\")\n",
    "            continue\n",
    "        \n",
    "        retrieved_docs = retrieval_function(query, db, k=k)\n",
    "        \n",
    "        # Count how many golden chunks are in the top k retrieved documents\n",
    "        chunks_found = 0\n",
    "        for golden_content in golden_contents:\n",
    "            for doc in retrieved_docs[:k]:\n",
    "                retrieved_content = doc['metadata'].get('original_content', doc['metadata'].get('content', '')).strip()\n",
    "                if retrieved_content == golden_content:\n",
    "                    chunks_found += 1\n",
    "                    break\n",
    "        \n",
    "        query_score = chunks_found / len(golden_contents)\n",
    "        total_score += query_score\n",
    "    \n",
    "    average_score = total_score / total_queries\n",
    "    pass_at_n = average_score * 100\n",
    "    return {\n",
    "        \"pass_at_n\": pass_at_n,\n",
    "        \"average_score\": average_score,\n",
    "        \"total_queries\": total_queries\n",
    "    }\n",
    "\n",
    "def retrieve_base(query: str, db, k: int = 20) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Retrieve relevant documents using either VectorDB or ContextualVectorDB.\n",
    "    \n",
    "    :param query: The query string\n",
    "    :param db: The VectorDB or ContextualVectorDB instance\n",
    "    :param k: Number of top results to retrieve\n",
    "    :return: List of retrieved documents\n",
    "    \"\"\"\n",
    "    return db.search(query, k=k)\n",
    "\n",
    "def evaluate_db(db, original_jsonl_path: str, k):\n",
    "    # Load the original JSONL data for queries and ground truth\n",
    "    original_data = load_jsonl(original_jsonl_path)\n",
    "    \n",
    "    # Evaluate retrieval\n",
    "    results = evaluate_retrieval(original_data, retrieve_base, db, k)\n",
    "    print(f\"Pass@{k}: {results['pass_at_n']:.2f}%\")\n",
    "    print(f\"Total Score: {results['average_score']}\")\n",
    "    print(f\"Total queries: {results['total_queries']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval: 100%|██████████| 248/248 [00:06<00:00, 40.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@5: 80.92%\n",
      "Total Score: 0.8091877880184332\n",
      "Total queries: 248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval: 100%|██████████| 248/248 [00:06<00:00, 39.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@10: 87.15%\n",
      "Total Score: 0.8714957757296468\n",
      "Total queries: 248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval: 100%|██████████| 248/248 [00:06<00:00, 39.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@20: 90.06%\n",
      "Total Score: 0.9006336405529954\n",
      "Total queries: 248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results5 = evaluate_db(base_db, 'data/evaluation_set.jsonl', 5)\n",
    "results10 = evaluate_db(base_db, 'data/evaluation_set.jsonl', 10)\n",
    "results20 = evaluate_db(base_db, 'data/evaluation_set.jsonl', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Embeddings\n",
    "\n",
    "With basic RAG, each embedded chunk contains a potentially useful piece of information, but these chunks lack context. With Contextual Embeddings, we create a variation on the embedding itself by adding more context to each text chunk before embedding it. Specifically, we use Claude to create a concise context that explains the chunk using the context of the overall document. In the case of our codebases dataset, we can provide both the chunk and the full file that each chunk was found within to an LLM, then produce the context. Then, we will combine this 'context' and the raw text chunk together into a single text block prior to creating each embedding.\n",
    "\n",
    "### Additional Considerations: Cost and Latency\n",
    "\n",
    "The extra work we're doing to 'situate' each document happens only at ingestion time: it's a cost you'll pay once when you store each document (and periodically in the future if you have a knowledge base that updates over time). There are many approaches like HyDE (hypothetical document embeddings) which involve performing steps to improve the representation of the query prior to executing a search. These techniques have shown to be moderately effective, but they add significant latency at runtime.\n",
    "\n",
    "[Prompt caching](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching) also makes this much more cost effective. Creating contextual embeddings requires us to pass the same document to the model for every chunk we want to generate extra context for. With prompt caching, we can write the overall doc to the cache once, and then because we're doing our ingestion job all in sequence, we can just read the document from cache as we generate context for each chunk within that document (the information you write to the cache has a 5 minute time to live). This means that the first time we pass a document to the model, we pay a bit more to write it to the cache, but for each subsequent API call that contains that doc, we receive  a 90% discount on all of the input tokens read from the cache. Assuming 800 token chunks, 8k token documents, 50 token context instructions, and 100 tokens of context per chunk, the cost to generate contextualized chunks is $1.02 per million document tokens.\n",
    "\n",
    "When you load data into your ContextualVectorDB below, you'll see in logs just how big this impact is. \n",
    "\n",
    "Warning: some smaller embedding models have a fixed input token limit. Contextualizing the chunk makes it longer, so if you notice much worse performance from contextualized embeddings, the contextualized chunk is likely getting truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Situated context: This chunk describes the `DiffExecutor` struct, which is an executor for differential fuzzing. It wraps two executors that are run sequentially with the same input, and also runs the secondary executor in the `run_target` method.\n",
      "Input tokens: 366\n",
      "Output tokens: 55\n",
      "Cache creation input tokens: 3046\n",
      "Cache read input tokens: 0\n"
     ]
    }
   ],
   "source": [
    "DOCUMENT_CONTEXT_PROMPT = \"\"\"\n",
    "<document>\n",
    "{doc_content}\n",
    "</document>\n",
    "\"\"\"\n",
    "\n",
    "CHUNK_CONTEXT_PROMPT = \"\"\"\n",
    "Here is the chunk we want to situate within the whole document\n",
    "<chunk>\n",
    "{chunk_content}\n",
    "</chunk>\n",
    "\n",
    "Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk.\n",
    "Answer only with the succinct context and nothing else.\n",
    "\"\"\"\n",
    "\n",
    "def situate_context(doc: str, chunk: str) -> str:\n",
    "    response = client.beta.prompt_caching.messages.create(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=1024,\n",
    "        temperature=0.0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": DOCUMENT_CONTEXT_PROMPT.format(doc_content=doc),\n",
    "                        \"cache_control\": {\"type\": \"ephemeral\"} #we will make use of prompt caching for the full documents\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": CHUNK_CONTEXT_PROMPT.format(chunk_content=chunk),\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        extra_headers={\"anthropic-beta\": \"prompt-caching-2024-07-31\"}\n",
    "    )\n",
    "    return response\n",
    "\n",
    "jsonl_data = load_jsonl('data/evaluation_set.jsonl')\n",
    "# Example usage\n",
    "doc_content = jsonl_data[0]['golden_documents'][0]['content']\n",
    "chunk_content = jsonl_data[0]['golden_chunks'][0]['content']\n",
    "\n",
    "response = situate_context(doc_content, chunk_content)\n",
    "print(f\"Situated context: {response.content[0].text}\")\n",
    "\n",
    "# Print cache performance metrics\n",
    "print(f\"Input tokens: {response.usage.input_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.output_tokens}\")\n",
    "print(f\"Cache creation input tokens: {response.usage.cache_creation_input_tokens}\")\n",
    "print(f\"Cache read input tokens: {response.usage.cache_read_input_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import voyageai\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "import anthropic\n",
    "import threading\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "class ContextualVectorDB:\n",
    "    def __init__(self, name: str, voyage_api_key=None, anthropic_api_key=None):\n",
    "        if voyage_api_key is None:\n",
    "            voyage_api_key = os.getenv(\"VOYAGE_API_KEY\")\n",
    "        if anthropic_api_key is None:\n",
    "            anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "        \n",
    "        self.voyage_client = voyageai.Client(api_key=voyage_api_key)\n",
    "        self.anthropic_client = anthropic.Anthropic(api_key=anthropic_api_key)\n",
    "        self.name = name\n",
    "        self.embeddings = []\n",
    "        self.metadata = []\n",
    "        self.query_cache = {}\n",
    "        self.db_path = f\"./data/{name}/contextual_vector_db.pkl\"\n",
    "\n",
    "        self.token_counts = {\n",
    "            'input': 0,\n",
    "            'output': 0,\n",
    "            'cache_read': 0,\n",
    "            'cache_creation': 0\n",
    "        }\n",
    "        self.token_lock = threading.Lock()\n",
    "\n",
    "    def situate_context(self, doc: str, chunk: str) -> tuple[str, Any]:\n",
    "        DOCUMENT_CONTEXT_PROMPT = \"\"\"\n",
    "        <document>\n",
    "        {doc_content}\n",
    "        </document>\n",
    "        \"\"\"\n",
    "\n",
    "        CHUNK_CONTEXT_PROMPT = \"\"\"\n",
    "        Here is the chunk we want to situate within the whole document\n",
    "        <chunk>\n",
    "        {chunk_content}\n",
    "        </chunk>\n",
    "\n",
    "        Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk.\n",
    "        Answer only with the succinct context and nothing else.\n",
    "        \"\"\"\n",
    "\n",
    "        response = self.anthropic_client.beta.prompt_caching.messages.create(\n",
    "            model=\"claude-3-haiku-20240307\",\n",
    "            max_tokens=1000,\n",
    "            temperature=0.0,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": DOCUMENT_CONTEXT_PROMPT.format(doc_content=doc),\n",
    "                            \"cache_control\": {\"type\": \"ephemeral\"} #we will make use of prompt caching for the full documents\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": CHUNK_CONTEXT_PROMPT.format(chunk_content=chunk),\n",
    "                        },\n",
    "                    ]\n",
    "                },\n",
    "            ],\n",
    "            extra_headers={\"anthropic-beta\": \"prompt-caching-2024-07-31\"}\n",
    "        )\n",
    "        return response.content[0].text, response.usage\n",
    "\n",
    "    def load_data(self, dataset: List[Dict[str, Any]], parallel_threads: int = 1):\n",
    "        if self.embeddings and self.metadata:\n",
    "            print(\"Vector database is already loaded. Skipping data loading.\")\n",
    "            return\n",
    "        if os.path.exists(self.db_path):\n",
    "            print(\"Loading vector database from disk.\")\n",
    "            self.load_db()\n",
    "            return\n",
    "\n",
    "        texts_to_embed = []\n",
    "        metadata = []\n",
    "        total_chunks = sum(len(doc['chunks']) for doc in dataset)\n",
    "\n",
    "        def process_chunk(doc, chunk):\n",
    "            #for each chunk, produce the context\n",
    "            contextualized_text, usage = self.situate_context(doc['content'], chunk['content'])\n",
    "            with self.token_lock:\n",
    "                self.token_counts['input'] += usage.input_tokens\n",
    "                self.token_counts['output'] += usage.output_tokens\n",
    "                self.token_counts['cache_read'] += usage.cache_read_input_tokens\n",
    "                self.token_counts['cache_creation'] += usage.cache_creation_input_tokens\n",
    "            \n",
    "            return {\n",
    "                #append the context to the original text chunk\n",
    "                'text_to_embed': f\"{chunk['content']}\\n\\n{contextualized_text}\",\n",
    "                'metadata': {\n",
    "                    'doc_id': doc['doc_id'],\n",
    "                    'original_uuid': doc['original_uuid'],\n",
    "                    'chunk_id': chunk['chunk_id'],\n",
    "                    'original_index': chunk['original_index'],\n",
    "                    'original_content': chunk['content'],\n",
    "                    'contextualized_content': contextualized_text\n",
    "                }\n",
    "            }\n",
    "\n",
    "        print(f\"Processing {total_chunks} chunks with {parallel_threads} threads\")\n",
    "        with ThreadPoolExecutor(max_workers=parallel_threads) as executor:\n",
    "            futures = []\n",
    "            for doc in dataset:\n",
    "                for chunk in doc['chunks']:\n",
    "                    futures.append(executor.submit(process_chunk, doc, chunk))\n",
    "            \n",
    "            for future in tqdm(as_completed(futures), total=total_chunks, desc=\"Processing chunks\"):\n",
    "                result = future.result()\n",
    "                texts_to_embed.append(result['text_to_embed'])\n",
    "                metadata.append(result['metadata'])\n",
    "\n",
    "        self._embed_and_store(texts_to_embed, metadata)\n",
    "        self.save_db()\n",
    "\n",
    "        #logging token usage\n",
    "        print(f\"Contextual Vector database loaded and saved. Total chunks processed: {len(texts_to_embed)}\")\n",
    "        print(f\"Total input tokens without caching: {self.token_counts['input']}\")\n",
    "        print(f\"Total output tokens: {self.token_counts['output']}\")\n",
    "        print(f\"Total input tokens written to cache: {self.token_counts['cache_creation']}\")\n",
    "        print(f\"Total input tokens read from cache: {self.token_counts['cache_read']}\")\n",
    "        \n",
    "        total_tokens = self.token_counts['input'] + self.token_counts['cache_read'] + self.token_counts['cache_creation']\n",
    "        savings_percentage = (self.token_counts['cache_read'] / total_tokens) * 100 if total_tokens > 0 else 0\n",
    "        print(f\"Total input token savings from prompt caching: {savings_percentage:.2f}% of all input tokens used were read from cache.\")\n",
    "        print(\"Tokens read from cache come at a 90 percent discount!\")\n",
    "\n",
    "    #we use voyage AI here for embeddings. Read more here: https://docs.voyageai.com/docs/embeddings\n",
    "    def _embed_and_store(self, texts: List[str], data: List[Dict[str, Any]]):\n",
    "        batch_size = 128\n",
    "        result = [\n",
    "            self.voyage_client.embed(\n",
    "                texts[i : i + batch_size],\n",
    "                model=\"voyage-2\"\n",
    "            ).embeddings\n",
    "            for i in range(0, len(texts), batch_size)\n",
    "        ]\n",
    "        self.embeddings = [embedding for batch in result for embedding in batch]\n",
    "        self.metadata = data\n",
    "\n",
    "    def search(self, query: str, k: int = 20) -> List[Dict[str, Any]]:\n",
    "        if query in self.query_cache:\n",
    "            query_embedding = self.query_cache[query]\n",
    "        else:\n",
    "            query_embedding = self.voyage_client.embed([query], model=\"voyage-2\").embeddings[0]\n",
    "            self.query_cache[query] = query_embedding\n",
    "\n",
    "        if not self.embeddings:\n",
    "            raise ValueError(\"No data loaded in the vector database.\")\n",
    "\n",
    "        similarities = np.dot(self.embeddings, query_embedding)\n",
    "        top_indices = np.argsort(similarities)[::-1][:k]\n",
    "        \n",
    "        top_results = []\n",
    "        for idx in top_indices:\n",
    "            result = {\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": float(similarities[idx]),\n",
    "            }\n",
    "            top_results.append(result)\n",
    "        return top_results\n",
    "\n",
    "    def save_db(self):\n",
    "        data = {\n",
    "            \"embeddings\": self.embeddings,\n",
    "            \"metadata\": self.metadata,\n",
    "            \"query_cache\": json.dumps(self.query_cache),\n",
    "        }\n",
    "        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)\n",
    "        with open(self.db_path, \"wb\") as file:\n",
    "            pickle.dump(data, file)\n",
    "\n",
    "    def load_db(self):\n",
    "        if not os.path.exists(self.db_path):\n",
    "            raise ValueError(\"Vector database file not found. Use load_data to create a new database.\")\n",
    "        with open(self.db_path, \"rb\") as file:\n",
    "            data = pickle.load(file)\n",
    "        self.embeddings = data[\"embeddings\"]\n",
    "        self.metadata = data[\"metadata\"]\n",
    "        self.query_cache = json.loads(data[\"query_cache\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 737 chunks with 5 threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 737/737 [02:37<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual Vector database loaded and saved. Total chunks processed: 737\n",
      "Total input tokens without caching: 500383\n",
      "Total output tokens: 40318\n",
      "Total input tokens written to cache: 341422\n",
      "Total input tokens read from cache: 2825073\n",
      "Total input token savings from prompt caching: 77.04% of all input tokens used were read from cache.\n",
      "Tokens read from cache come at a 90 percent discount!\n"
     ]
    }
   ],
   "source": [
    "# Load the transformed dataset\n",
    "with open('data/codebase_chunks.json', 'r') as f:\n",
    "    transformed_dataset = json.load(f)\n",
    "\n",
    "# Initialize the ContextualVectorDB\n",
    "contextual_db = ContextualVectorDB(\"my_contextual_db\")\n",
    "\n",
    "# Load and process the data\n",
    "#note: consider increasing the number of parallel threads to run this faster, or reducing the number of parallel threads if concerned about hitting your API rate limit\n",
    "contextual_db.load_data(transformed_dataset, parallel_threads=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval: 100%|██████████| 248/248 [00:06<00:00, 39.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@5: 86.37%\n",
      "Total Score: 0.8637192780337941\n",
      "Total queries: 248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval: 100%|██████████| 248/248 [00:06<00:00, 40.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@10: 92.81%\n",
      "Total Score: 0.9280913978494625\n",
      "Total queries: 248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval: 100%|██████████| 248/248 [00:06<00:00, 39.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@20: 93.78%\n",
      "Total Score: 0.9378360215053763\n",
      "Total queries: 248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "r5 = evaluate_db(contextual_db, 'data/evaluation_set.jsonl', 5)\n",
    "r10 = evaluate_db(contextual_db, 'data/evaluation_set.jsonl', 10)\n",
    "r20 = evaluate_db(contextual_db, 'data/evaluation_set.jsonl', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual BM25\n",
    "\n",
    "Contextual embeddings is an improvement on traditional semantic search RAG, but we can improve performance further. In this section we'll show you how you can use contextual embeddings and *contextual* BM25 together. While you can see performance gains by pairing these techniques together without the context, adding context to these methods reduces the top-20-chunk retrieval failure rate by 42%.\n",
    "\n",
    "BM25 is a probabilistic ranking function that improves upon TF-IDF. It scores documents based on query term frequency, while accounting for document length and term saturation. BM25 is widely used in modern search engines for its effectiveness in ranking relevant documents. For more details, see [this blog post](https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables). We'll use elastic search for the BM25 portion of this section, which will require you to have the elasticsearch library installed and it will also require you to spin up an Elasticsearch server in the background. The easiest way to do this is to install [docker](https://docs.docker.com/engine/install/) and run the following docker command:\n",
    "\n",
    "`docker run -d --name elasticsearch -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" -e \"xpack.security.enabled=false\" elasticsearch:8.8.0`\n",
    "\n",
    "One difference between a typical BM25 search and what we'll do in this section is that, for each chunk, we'll run each BM25 search on both the chunk content and the additional context that we generated in the previous section. From there, we'll use a technique called reciprocal rank fusion to merge the results from our BM25 search with our semantic search results. This allows us to perform a hybrid search across both our BM25 corpus and vector DB to return the most optimal documents for a given query.\n",
    "\n",
    "In the function below, we allow you the option to add weightings to the semantic search and BM25 search documents as you merge them with Reciprocal Rank Fusion. By default, we set these to 0.8 for the semantic search results and 0.2 to the BM25 results. We'd encourage you to experiment with different values here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "\n",
    "class ElasticsearchBM25:\n",
    "    def __init__(self, index_name: str = \"contextual_bm25_index\"):\n",
    "        self.es_client = Elasticsearch(\"http://localhost:9200\")\n",
    "        self.index_name = index_name\n",
    "        self.create_index()\n",
    "\n",
    "    def create_index(self):\n",
    "        index_settings = {\n",
    "            \"settings\": {\n",
    "                \"analysis\": {\"analyzer\": {\"default\": {\"type\": \"english\"}}},\n",
    "                \"similarity\": {\"default\": {\"type\": \"BM25\"}},\n",
    "                \"index.queries.cache.enabled\": False  # Disable query cache\n",
    "            },\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"content\": {\"type\": \"text\", \"analyzer\": \"english\"},\n",
    "                    \"contextualized_content\": {\"type\": \"text\", \"analyzer\": \"english\"},\n",
    "                    \"doc_id\": {\"type\": \"keyword\", \"index\": False},\n",
    "                    \"chunk_id\": {\"type\": \"keyword\", \"index\": False},\n",
    "                    \"original_index\": {\"type\": \"integer\", \"index\": False},\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "        if not self.es_client.indices.exists(index=self.index_name):\n",
    "            self.es_client.indices.create(index=self.index_name, body=index_settings)\n",
    "            print(f\"Created index: {self.index_name}\")\n",
    "\n",
    "    def index_documents(self, documents: List[Dict[str, Any]]):\n",
    "        actions = [\n",
    "            {\n",
    "                \"_index\": self.index_name,\n",
    "                \"_source\": {\n",
    "                    \"content\": doc[\"original_content\"],\n",
    "                    \"contextualized_content\": doc[\"contextualized_content\"],\n",
    "                    \"doc_id\": doc[\"doc_id\"],\n",
    "                    \"chunk_id\": doc[\"chunk_id\"],\n",
    "                    \"original_index\": doc[\"original_index\"],\n",
    "                },\n",
    "            }\n",
    "            for doc in documents\n",
    "        ]\n",
    "        success, _ = bulk(self.es_client, actions)\n",
    "        self.es_client.indices.refresh(index=self.index_name)\n",
    "        return success\n",
    "\n",
    "    def search(self, query: str, k: int = 20) -> List[Dict[str, Any]]:\n",
    "        self.es_client.indices.refresh(index=self.index_name)  # Force refresh before each search\n",
    "        search_body = {\n",
    "            \"query\": {\n",
    "                \"multi_match\": {\n",
    "                    \"query\": query,\n",
    "                    \"fields\": [\"content\", \"contextualized_content\"],\n",
    "                }\n",
    "            },\n",
    "            \"size\": k,\n",
    "        }\n",
    "        response = self.es_client.search(index=self.index_name, body=search_body)\n",
    "        return [\n",
    "            {\n",
    "                \"doc_id\": hit[\"_source\"][\"doc_id\"],\n",
    "                \"original_index\": hit[\"_source\"][\"original_index\"],\n",
    "                \"content\": hit[\"_source\"][\"content\"],\n",
    "                \"contextualized_content\": hit[\"_source\"][\"contextualized_content\"],\n",
    "                \"score\": hit[\"_score\"],\n",
    "            }\n",
    "            for hit in response[\"hits\"][\"hits\"]\n",
    "        ]\n",
    "    \n",
    "def create_elasticsearch_bm25_index(db: ContextualVectorDB):\n",
    "    es_bm25 = ElasticsearchBM25()\n",
    "    es_bm25.index_documents(db.metadata)\n",
    "    return es_bm25\n",
    "\n",
    "def retrieve_advanced(query: str, db: ContextualVectorDB, es_bm25: ElasticsearchBM25, k: int, semantic_weight: float = 0.8, bm25_weight: float = 0.2):\n",
    "    num_chunks_to_recall = 150\n",
    "\n",
    "    # Semantic search\n",
    "    semantic_results = db.search(query, k=num_chunks_to_recall)\n",
    "    ranked_chunk_ids = [(result['metadata']['doc_id'], result['metadata']['original_index']) for result in semantic_results]\n",
    "\n",
    "    # BM25 search using Elasticsearch\n",
    "    bm25_results = es_bm25.search(query, k=num_chunks_to_recall)\n",
    "    ranked_bm25_chunk_ids = [(result['doc_id'], result['original_index']) for result in bm25_results]\n",
    "\n",
    "    # Combine results\n",
    "    chunk_ids = list(set(ranked_chunk_ids + ranked_bm25_chunk_ids))\n",
    "    chunk_id_to_score = {}\n",
    "\n",
    "    # Initial scoring with weights\n",
    "    for chunk_id in chunk_ids:\n",
    "        score = 0\n",
    "        if chunk_id in ranked_chunk_ids:\n",
    "            index = ranked_chunk_ids.index(chunk_id)\n",
    "            score += semantic_weight * (1 / (index + 1))  # Weighted 1/n scoring for semantic\n",
    "        if chunk_id in ranked_bm25_chunk_ids:\n",
    "            index = ranked_bm25_chunk_ids.index(chunk_id)\n",
    "            score += bm25_weight * (1 / (index + 1))  # Weighted 1/n scoring for BM25\n",
    "        chunk_id_to_score[chunk_id] = score\n",
    "\n",
    "    # Sort chunk IDs by their scores in descending order\n",
    "    sorted_chunk_ids = sorted(\n",
    "        chunk_id_to_score.keys(), key=lambda x: (chunk_id_to_score[x], x[0], x[1]), reverse=True\n",
    "    )\n",
    "\n",
    "    # Assign new scores based on the sorted order\n",
    "    for index, chunk_id in enumerate(sorted_chunk_ids):\n",
    "        chunk_id_to_score[chunk_id] = 1 / (index + 1)\n",
    "\n",
    "    # Prepare the final results\n",
    "    final_results = []\n",
    "    semantic_count = 0\n",
    "    bm25_count = 0\n",
    "    for chunk_id in sorted_chunk_ids[:k]:\n",
    "        chunk_metadata = next(chunk for chunk in db.metadata if chunk['doc_id'] == chunk_id[0] and chunk['original_index'] == chunk_id[1])\n",
    "        is_from_semantic = chunk_id in ranked_chunk_ids\n",
    "        is_from_bm25 = chunk_id in ranked_bm25_chunk_ids\n",
    "        final_results.append({\n",
    "            'chunk': chunk_metadata,\n",
    "            'score': chunk_id_to_score[chunk_id],\n",
    "            'from_semantic': is_from_semantic,\n",
    "            'from_bm25': is_from_bm25\n",
    "        })\n",
    "        \n",
    "        if is_from_semantic and not is_from_bm25:\n",
    "            semantic_count += 1\n",
    "        elif is_from_bm25 and not is_from_semantic:\n",
    "            bm25_count += 1\n",
    "        else:  # it's in both\n",
    "            semantic_count += 0.5\n",
    "            bm25_count += 0.5\n",
    "\n",
    "    return final_results, semantic_count, bm25_count\n",
    "\n",
    "def load_jsonl(file_path: str) -> List[Dict[str, Any]]:\n",
    "    with open(file_path, 'r') as file:\n",
    "        return [json.loads(line) for line in file]\n",
    "\n",
    "def evaluate_db_advanced(db: ContextualVectorDB, original_jsonl_path: str, k: int):\n",
    "    original_data = load_jsonl(original_jsonl_path)\n",
    "    es_bm25 = create_elasticsearch_bm25_index(db)\n",
    "    \n",
    "    try:\n",
    "        # Warm-up queries\n",
    "        warm_up_queries = original_data[:10]\n",
    "        for query_item in warm_up_queries:\n",
    "            _ = retrieve_advanced(query_item['query'], db, es_bm25, k)\n",
    "        \n",
    "        total_score = 0\n",
    "        total_semantic_count = 0\n",
    "        total_bm25_count = 0\n",
    "        total_results = 0\n",
    "        \n",
    "        for query_item in tqdm(original_data, desc=\"Evaluating retrieval\"):\n",
    "            query = query_item['query']\n",
    "            golden_chunk_uuids = query_item['golden_chunk_uuids']\n",
    "            \n",
    "            golden_contents = []\n",
    "            for doc_uuid, chunk_index in golden_chunk_uuids:\n",
    "                golden_doc = next((doc for doc in query_item['golden_documents'] if doc['uuid'] == doc_uuid), None)\n",
    "                if golden_doc:\n",
    "                    golden_chunk = next((chunk for chunk in golden_doc['chunks'] if chunk['index'] == chunk_index), None)\n",
    "                    if golden_chunk:\n",
    "                        golden_contents.append(golden_chunk['content'].strip())\n",
    "            \n",
    "            if not golden_contents:\n",
    "                print(f\"Warning: No golden contents found for query: {query}\")\n",
    "                continue\n",
    "            \n",
    "            retrieved_docs, semantic_count, bm25_count = retrieve_advanced(query, db, es_bm25, k)\n",
    "            \n",
    "            chunks_found = 0\n",
    "            for golden_content in golden_contents:\n",
    "                for doc in retrieved_docs[:k]:\n",
    "                    retrieved_content = doc['chunk']['original_content'].strip()\n",
    "                    if retrieved_content == golden_content:\n",
    "                        chunks_found += 1\n",
    "                        break\n",
    "            \n",
    "            query_score = chunks_found / len(golden_contents)\n",
    "            total_score += query_score\n",
    "            \n",
    "            total_semantic_count += semantic_count\n",
    "            total_bm25_count += bm25_count\n",
    "            total_results += len(retrieved_docs)\n",
    "        \n",
    "        total_queries = len(original_data)\n",
    "        average_score = total_score / total_queries\n",
    "        pass_at_n = average_score * 100\n",
    "        \n",
    "        semantic_percentage = (total_semantic_count / total_results) * 100 if total_results > 0 else 0\n",
    "        bm25_percentage = (total_bm25_count / total_results) * 100 if total_results > 0 else 0\n",
    "        \n",
    "        results = {\n",
    "            \"pass_at_n\": pass_at_n,\n",
    "            \"average_score\": average_score,\n",
    "            \"total_queries\": total_queries\n",
    "        }\n",
    "        \n",
    "        print(f\"Pass@{k}: {pass_at_n:.2f}%\")\n",
    "        print(f\"Average Score: {average_score:.2f}\")\n",
    "        print(f\"Total queries: {total_queries}\")\n",
    "        print(f\"Percentage of results from semantic search: {semantic_percentage:.2f}%\")\n",
    "        print(f\"Percentage of results from BM25: {bm25_percentage:.2f}%\")\n",
    "        \n",
    "        return results, {\"semantic\": semantic_percentage, \"bm25\": bm25_percentage}\n",
    "    \n",
    "    finally:\n",
    "        # Delete the Elasticsearch index\n",
    "        if es_bm25.es_client.indices.exists(index=es_bm25.index_name):\n",
    "            es_bm25.es_client.indices.delete(index=es_bm25.index_name)\n",
    "            print(f\"Deleted Elasticsearch index: {es_bm25.index_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created index: contextual_bm25_index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval: 100%|██████████| 248/248 [00:08<00:00, 28.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@5: 86.43%\n",
      "Average Score: 0.86\n",
      "Total queries: 248\n",
      "Percentage of results from semantic search: 55.12%\n",
      "Percentage of results from BM25: 44.88%\n",
      "Deleted Elasticsearch index: contextual_bm25_index\n",
      "Created index: contextual_bm25_index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval: 100%|██████████| 248/248 [00:08<00:00, 28.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@10: 93.21%\n",
      "Average Score: 0.93\n",
      "Total queries: 248\n",
      "Percentage of results from semantic search: 58.35%\n",
      "Percentage of results from BM25: 41.65%\n",
      "Deleted Elasticsearch index: contextual_bm25_index\n",
      "Created index: contextual_bm25_index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval: 100%|██████████| 248/248 [00:08<00:00, 28.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@20: 94.99%\n",
      "Average Score: 0.95\n",
      "Total queries: 248\n",
      "Percentage of results from semantic search: 61.94%\n",
      "Percentage of results from BM25: 38.06%\n",
      "Deleted Elasticsearch index: contextual_bm25_index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results5 = evaluate_db_advanced(contextual_db, 'data/evaluation_set.jsonl', 5)\n",
    "results10 = evaluate_db_advanced(contextual_db, 'data/evaluation_set.jsonl', 10)\n",
    "results20 = evaluate_db_advanced(contextual_db, 'data/evaluation_set.jsonl', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a Reranking Step\n",
    "\n",
    "If you want to improve performance further, we recommend adding a re-ranking step. When using a re-ranker, you can retrieve more documents initially from your vector store, then use your re-ranker to select a subset of these documents. One common technique is to use re-ranking as a way to implement high precision hybrid search. You can use a combination of semantic search and keyword based search in your initial retrieval step (as we have done earlier in this guide), then use a re-ranking step to choose only the k most relevant docs from a combined list of documents returned by your semantic search and keyword search systems.\n",
    "\n",
    "Below, we'll demonstrate only the re-ranking step (skipping the hybrid search technique for now). You'll see that we retrieve 10x the number of documents than the number of final k documents we want to retrieve, then use a re-ranking model from Cohere to select the 10 most relevant results from that list. Adding the re-ranking step delivers a modest additional gain in performance. In our case, Pass@10 improves from 92.81% --> 94.79%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "from typing import List, Dict, Any, Callable\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_jsonl(file_path: str) -> List[Dict[str, Any]]:\n",
    "    with open(file_path, 'r') as file:\n",
    "        return [json.loads(line) for line in file]\n",
    "\n",
    "def chunk_to_content(chunk: Dict[str, Any]) -> str:\n",
    "    original_content = chunk['metadata']['original_content']\n",
    "    contextualized_content = chunk['metadata']['contextualized_content']\n",
    "    return f\"{original_content}\\n\\nContext: {contextualized_content}\" \n",
    "\n",
    "def retrieve_rerank(query: str, db, k: int) -> List[Dict[str, Any]]:\n",
    "    co = cohere.Client( os.getenv(\"COHERE_API_KEY\"))\n",
    "    \n",
    "    # Retrieve more results than we normally would\n",
    "    semantic_results = db.search(query, k=k*10)\n",
    "    \n",
    "    # Extract documents for reranking, using the contextualized content\n",
    "    documents = [chunk_to_content(res) for res in semantic_results]\n",
    "\n",
    "    response = co.rerank(\n",
    "        model=\"rerank-english-v3.0\",\n",
    "        query=query,\n",
    "        documents=documents,\n",
    "        top_n=k\n",
    "    )\n",
    "    time.sleep(0.1)\n",
    "    \n",
    "    final_results = []\n",
    "    for r in response.results:\n",
    "        original_result = semantic_results[r.index]\n",
    "        final_results.append({\n",
    "            \"chunk\": original_result['metadata'],\n",
    "            \"score\": r.relevance_score\n",
    "        })\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "def evaluate_retrieval_rerank(queries: List[Dict[str, Any]], retrieval_function: Callable, db, k: int = 20) -> Dict[str, float]:\n",
    "    total_score = 0\n",
    "    total_queries = len(queries)\n",
    "    \n",
    "    for query_item in tqdm(queries, desc=\"Evaluating retrieval\"):\n",
    "        query = query_item['query']\n",
    "        golden_chunk_uuids = query_item['golden_chunk_uuids']\n",
    "        \n",
    "        golden_contents = []\n",
    "        for doc_uuid, chunk_index in golden_chunk_uuids:\n",
    "            golden_doc = next((doc for doc in query_item['golden_documents'] if doc['uuid'] == doc_uuid), None)\n",
    "            if golden_doc:\n",
    "                golden_chunk = next((chunk for chunk in golden_doc['chunks'] if chunk['index'] == chunk_index), None)\n",
    "                if golden_chunk:\n",
    "                    golden_contents.append(golden_chunk['content'].strip())\n",
    "        \n",
    "        if not golden_contents:\n",
    "            print(f\"Warning: No golden contents found for query: {query}\")\n",
    "            continue\n",
    "        \n",
    "        retrieved_docs = retrieval_function(query, db, k)\n",
    "        \n",
    "        chunks_found = 0\n",
    "        for golden_content in golden_contents:\n",
    "            for doc in retrieved_docs[:k]:\n",
    "                retrieved_content = doc['chunk']['original_content'].strip()\n",
    "                if retrieved_content == golden_content:\n",
    "                    chunks_found += 1\n",
    "                    break\n",
    "        \n",
    "        query_score = chunks_found / len(golden_contents)\n",
    "        total_score += query_score\n",
    "    \n",
    "    average_score = total_score / total_queries\n",
    "    pass_at_n = average_score * 100\n",
    "    return {\n",
    "        \"pass_at_n\": pass_at_n,\n",
    "        \"average_score\": average_score,\n",
    "        \"total_queries\": total_queries\n",
    "    }\n",
    "\n",
    "def evaluate_db_advanced(db, original_jsonl_path, k):\n",
    "    original_data = load_jsonl(original_jsonl_path)\n",
    "    \n",
    "    def retrieval_function(query, db, k):\n",
    "        return retrieve_rerank(query, db, k)\n",
    "    \n",
    "    results = evaluate_retrieval_rerank(original_data, retrieval_function, db, k)\n",
    "    print(f\"Pass@{k}: {results['pass_at_n']:.2f}%\")\n",
    "    print(f\"Average Score: {results['average_score']}\")\n",
    "    print(f\"Total queries: {results['total_queries']}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval: 100%|██████████| 248/248 [01:22<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@5: 91.24%\n",
      "Average Score: 0.912442396313364\n",
      "Total queries: 248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval: 100%|██████████| 248/248 [01:34<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@10: 94.79%\n",
      "Average Score: 0.9479166666666667\n",
      "Total queries: 248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrieval: 100%|██████████| 248/248 [02:08<00:00,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@20: 96.30%\n",
      "Average Score: 0.9630376344086022\n",
      "Total queries: 248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results5 = evaluate_db_advanced(contextual_db, 'data/evaluation_set.jsonl', 5)\n",
    "results10 = evaluate_db_advanced(contextual_db, 'data/evaluation_set.jsonl', 10)\n",
    "results20 = evaluate_db_advanced(contextual_db, 'data/evaluation_set.jsonl', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps and Key Takeaways\n",
    "\n",
    "1) We demonstrated how to use Contextual Embeddings to improve retrieval performance, then delivered additional improvements with Contextual BM25 and reranking.\n",
    "\n",
    "2) This example used codebases, but these methods also apply to other data types such as internal company knowledge bases, financial & legal content, educational content, and much more. \n",
    "\n",
    "3) If you are an AWS user, you can get started with the Lambda function in `contextual-rag-lambda-function`, and if you're a GCP user you can spin up your own Cloud Run instance and follow a similar pattern!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
